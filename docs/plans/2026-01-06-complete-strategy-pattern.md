# Complete Strategy Pattern Implementation

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:subagent-driven-development to implement this plan task-by-task.

**Goal:** Build the Strategy Pattern architecture (eval + hemdov modules) to enable Tasks 3-5 from silent failures plan and complete the DSPy prompt improvement backend.

**Architecture:** Hexagonal/Clean Architecture with Strategy Pattern - 3 concrete strategies (Simple/Moderate/Complex) selected by ComplexityAnalyzer using multi-dimensional scoring.

**Tech Stack:** Python 3.14, DSPy 2.6, FastAPI, pytest, dataclasses/enums, type hints

---

## Task 1: Create eval package structure and ComplexityLevel enum

**Files:**
- Create: `eval/__init__.py`
- Create: `eval/src/__init__.py`
- Create: `eval/src/complexity_analyzer.py`
- Test: `tests/test_complexity_analyzer.py`

**Step 1: Write package init files**

```bash
mkdir -p eval/src
touch eval/__init__.py
touch eval/src/__init__.py
```

**Step 2: Write ComplexityLevel enum and basic analyzer**

```python
# eval/src/complexity_analyzer.py
from enum import Enum
from typing import Tuple
import re


class ComplexityLevel(Enum):
    """Complexity levels for prompt classification."""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"


class ComplexityAnalyzer:
    """Analyzes prompt complexity using multi-dimensional scoring."""

    # Technical terms that indicate complexity
    TECHNICAL_TERMS = [
        'api', 'http', 'json', 'sql', 'docker', 'kubernetes',
        'algorithm', 'function', 'class', 'async', 'await',
        'frontend', 'backend', 'database', 'authentication'
    ]

    def analyze(self, original_idea: str, context: str) -> ComplexityLevel:
        """
        Analyze prompt complexity based on multiple dimensions.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            ComplexityLevel classification

        Raises:
            TypeError: If inputs are not strings
            ValueError: If inputs are empty
        """
        if not isinstance(original_idea, str):
            raise TypeError(f"original_idea must be str, got {type(original_idea)}")
        if not isinstance(context, str):
            raise TypeError(f"context must be str, got {type(context)}")

        if not original_idea.strip() and not context.strip():
            raise ValueError("Both original_idea and context cannot be empty")

        combined = f"{original_idea} {context}"
        length = len(combined)
        technical_score = self._calculate_technical_score(combined)

        # Decision logic
        if length <= 50 and technical_score == 0:
            return ComplexityLevel.SIMPLE
        elif length <= 150 and technical_score <= 2:
            return ComplexityLevel.MODERATE
        else:
            return ComplexityLevel.COMPLEX

    def _calculate_technical_score(self, text: str) -> int:
        """Count technical terms using word boundaries."""
        count = 0
        text_lower = text.lower()
        for term in self.TECHNICAL_TERMS:
            # Word boundary matching to avoid false positives
            pattern = r'\b' + re.escape(term) + r'\b'
            if re.search(pattern, text_lower):
                count += 1
        return min(count, 5)  # Cap at 5 for normalization
```

**Step 3: Write tests**

```python
# tests/test_complexity_analyzer.py
import pytest
from eval.src.complexity_analyzer import ComplexityAnalyzer, ComplexityLevel


def test_analyzer_simple_by_length():
    analyzer = ComplexityAnalyzer()
    result = analyzer.analyze("hola mundo", "")
    assert result == ComplexityLevel.SIMPLE


def test_analyzer_moderate_by_length():
    analyzer = ComplexityAnalyzer()
    result = analyzer.analyze("crea un componente Button con variantes", "")
    assert result == ComplexityLevel.MODERATE


def test_analyzer_complex_by_length():
    analyzer = ComplexityAnalyzer()
    long_input = "dise簽a " * 50  # > 150 chars
    result = analyzer.analyze(long_input, "")
    assert result == ComplexityLevel.COMPLEX


def test_analyzer_technical_terms():
    analyzer = ComplexityAnalyzer()
    result = analyzer.analyze("create API endpoint with authentication", "")
    assert result == ComplexityLevel.COMPLEX


def test_analyzer_input_validation():
    analyzer = ComplexityAnalyzer()
    with pytest.raises(TypeError):
        analyzer.analyze(123, "")
    with pytest.raises(ValueError):
        analyzer.analyze("", "")
```

**Step 4: Run tests to verify they fail**

```bash
pytest tests/test_complexity_analyzer.py -v
```
Expected: FAIL (modules don't exist yet)

**Step 5: Create files and run tests to verify they pass**

```bash
pytest tests/test_complexity_analyzer.py -v
```
Expected: PASS

**Step 6: Commit**

```bash
git add eval/ tests/test_complexity_analyzer.py
git commit -m "feat: add ComplexityAnalyzer with multi-dimensional scoring

- Add ComplexityLevel enum (SIMPLE, MODERATE, COMPLEX)
- Implement ComplexityAnalyzer with length and technical term scoring
- Add input validation (TypeError, ValueError)
- Add comprehensive tests

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 2: Create Strategy base class and interface

**Files:**
- Create: `eval/src/strategies/__init__.py`
- Create: `eval/src/strategies/base.py`
- Test: `tests/test_strategies/test_base.py`

**Step 1: Write failing test for base class**

```python
# tests/test_strategies/test_base.py
import pytest
from eval.src.strategies.base import PromptImproverStrategy


def test_base_strategy_has_required_methods():
    """Test that base strategy defines the interface."""
    assert hasattr(PromptImproverStrategy, 'improve')
    assert hasattr(PromptImproverStrategy, 'name')


def test_base_strategy_cannot_be_instantiated():
    """Test that base class is abstract."""
    from eval.src.strategies.base import PromptImproverStrategy
    from eval.src.dspy_prompt_improver import PromptImprover

    strategy = PromptImproverStrategy(PromptImprover())
    assert strategy.name == "base"
    assert strategy.max_length == 5000


def test_base_strategy_validates_input():
    """Test that base strategy validates input types."""
    from eval.src.strategies.base import PromptImproverStrategy
    from eval.src.dspy_prompt_improver import PromptImprover

    strategy = PromptImproverStrategy(PromptImprover())

    with pytest.raises(TypeError, match="original_idea must be str"):
        strategy.improve(123, "")

    with pytest.raises(ValueError, match="original_idea cannot be empty"):
        strategy.improve("", "")
```

**Step 2: Run tests to verify they fail**

```bash
pytest tests/test_strategies/test_base.py -v
```
Expected: FAIL (files don't exist yet)

**Step 3: Implement base strategy class**

```python
# eval/src/strategies/base.py
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from eval.src.dspy_prompt_improver import PromptImprover


class PromptImproverStrategy(ABC):
    """
    Abstract base class for prompt improvement strategies.

    Each strategy defines how to improve prompts with specific approaches
    (zero-shot, few-shot, chain-of-thought, etc.).
    """

    def __init__(self, improver: 'PromptImprover'):
        """
        Initialize strategy with a DSPy PromptImprover instance.

        Args:
            improver: The DSPy PromptImprover to use
        """
        if improver is None:
            raise ValueError("improver cannot be None")
        self._improver = improver

    @property
    @abstractmethod
    def name(self) -> str:
        """Strategy name for logging and identification."""
        pass

    @property
    @abstractmethod
    def max_length(self) -> int:
        """Maximum output length for this strategy."""
        pass

    def improve(self, original_idea: str, context: str) -> str:
        """
        Improve the given prompt using this strategy's approach.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            Improved prompt text

        Raises:
            TypeError: If inputs are not strings
            ValueError: If inputs are empty
            RuntimeError: If DSPy improvement fails
        """
        if not isinstance(original_idea, str):
            raise TypeError(f"original_idea must be str, got {type(original_idea)}")
        if not isinstance(context, str):
            raise TypeError(f"context must be str, got {type(context)}")

        if not original_idea.strip():
            raise ValueError("original_idea cannot be empty")

        try:
            result = self._improve_with_strategy(original_idea, context)
            # Truncate to max_length if needed
            if len(result) > self.max_length:
                result = self._truncate_at_sentence(result, self.max_length)
            return result
        except Exception as e:
            raise RuntimeError(f"Strategy {self.name} failed: {e}") from e

    @abstractmethod
    def _improve_with_strategy(self, original_idea: str, context: str) -> str:
        """
        Strategy-specific implementation.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            Improved prompt text
        """
        pass

    def _truncate_at_sentence(self, text: str, max_len: int) -> str:
        """Truncate text at nearest sentence boundary."""
        if len(text) <= max_len:
            return text

        # Try to truncate at sentence ending
        for delimiter in ['.', '!', '?', '\n']:
            last_pos = text.rfind(delimiter, 0, max_len)
            if last_pos > max_len * 0.7:  # At least 70% of max
                return text[:last_pos + 1].strip()

        # Fallback: truncate at max_len
        return text[:max_len].rstrip()
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_strategies/test_base.py -v
```
Expected: Some FAIL (PromptImprover doesn't exist yet - we'll create it in Task 5)

**Step 5: Commit**

```bash
git add eval/src/strategies/ tests/test_strategies/
git commit -m "feat: add PromptImproverStrategy base class

- Define abstract interface for all strategies
- Add input validation and error handling
- Add max_length enforcement with sentence boundary truncation
- Add base strategy tests

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 3: Create SimpleStrategy (zero-shot, ultra-concise)

**Files:**
- Create: `eval/src/strategies/simple_strategy.py`
- Test: `tests/test_strategies/test_simple_strategy.py`

**Step 1: Write failing test**

```python
# tests/test_strategies/test_simple_strategy.py
import pytest
from eval.src.strategies.simple_strategy import SimpleStrategy
from eval.src.complexity_analyzer import ComplexityLevel


def test_simple_strategy_name():
    strategy = SimpleStrategy(None)  # improver will be mocked
    assert strategy.name == "simple"


def test_simple_strategy_max_length():
    strategy = SimpleStrategy(None)
    assert strategy.max_length == 800


def test_simple_strategy_improves_short_prompt():
    from unittest.mock import Mock
    improver = Mock()
    improver.zero_shot_improve = Mock(return_value="Improved: Hola mundo")

    strategy = SimpleStrategy(improver)
    result = strategy.improve("hola mundo", "")
    assert result == "Improved: Hola mundo"


def test_simple_strategy_truncates_long_output():
    from unittest.mock import Mock
    improver = Mock()
    long_result = "A" * 1000
    improver.zero_shot_improve = Mock(return_value=long_result)

    strategy = SimpleStrategy(improver)
    result = strategy.improve("test", "")
    assert len(result) == 800
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_strategies/test_simple_strategy.py::test_simple_strategy_name -v
```
Expected: FAIL

**Step 3: Implement SimpleStrategy**

```python
# eval/src/strategies/simple_strategy.py
from .base import PromptImproverStrategy


class SimpleStrategy(PromptImproverStrategy):
    """
    Zero-shot strategy for simple, short prompts.

    Characteristics:
    - No few-shot examples (fast, minimal latency)
    - 800 character output limit (ultra-concise)
    - Best for: quick commands, simple questions
    """

    @property
    def name(self) -> str:
        return "simple"

    @property
    def max_length(self) -> int:
        return 800

    def _improve_with_strategy(self, original_idea: str, context: str) -> str:
        """
        Use zero-shot DSPy improvement.

        Args:
            original_idea: The user's input prompt
            context: Additional context (typically empty for simple prompts)

        Returns:
            Improved prompt
        """
        return self._improver.zero_shot_improve(original_idea, context)
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_strategies/test_simple_strategy.py -v
```
Expected: PASS (after PromptImprover.zero_shot_improve exists)

**Step 5: Commit**

```bash
git add eval/src/strategies/simple_strategy.py tests/test_strategies/test_simple_strategy.py
git commit -m "feat: add SimpleStrategy for zero-shot improvement

- Ultra-concise strategy with 800 char limit
- Zero-shot approach (no few-shot examples)
- Best for quick commands and simple prompts

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 4: Create ModerateStrategy (ChainOfThought, balanced)

**Files:**
- Create: `eval/src/strategies/moderate_strategy.py`
- Test: `tests/test_strategies/test_moderate_strategy.py`

**Step 1: Write failing test**

```python
# tests/test_strategies/test_moderate_strategy.py
import pytest
from eval.src.strategies.moderate_strategy import ModerateStrategy


def test_moderate_strategy_name():
    strategy = ModerateStrategy(None)
    assert strategy.name == "moderate"


def test_moderate_strategy_max_length():
    strategy = ModerateStrategy(None)
    assert strategy.max_length == 2000


def test_moderate_strategy_uses_chain_of_thought():
    from unittest.mock import Mock
    improver = Mock()
    improver.chain_of_thought_improve = Mock(return_value="Improved with CoT")

    strategy = ModerateStrategy(improver)
    result = strategy.improve("create button component", "with variants")
    assert result == "Improved with CoT"
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_strategies/test_moderate_strategy.py::test_moderate_strategy_name -v
```
Expected: FAIL

**Step 3: Implement ModerateStrategy**

```python
# eval/src/strategies/moderate_strategy.py
from .base import PromptImproverStrategy


class ModerateStrategy(PromptImproverStrategy):
    """
    Chain-of-Thought strategy for moderate complexity prompts.

    Characteristics:
    - Chain-of-Thought reasoning (step-by-step)
    - 2000 character output limit (balanced detail)
    - Best for: component design, feature implementation
    """

    @property
    def name(self) -> str:
        return "moderate"

    @property
    def max_length(self) -> int:
        return 2000

    def _improve_with_strategy(self, original_idea: str, context: str) -> str:
        """
        Use Chain-of-Thought DSPy improvement.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            Improved prompt with reasoning
        """
        return self._improver.chain_of_thought_improve(original_idea, context)
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_strategies/test_moderate_strategy.py -v
```
Expected: PASS

**Step 5: Commit**

```bash
git add eval/src/strategies/moderate_strategy.py tests/test_strategies/test_moderate_strategy.py
git commit -m "feat: add ModerateStrategy with Chain-of-Thought

- Balanced strategy with 2000 char limit
- Chain-of-Thought approach for structured reasoning
- Best for component design and feature implementation

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 5: Create ComplexStrategy (KNNFewShot, comprehensive)

**Files:**
- Create: `eval/src/strategies/complex_strategy.py`
- Test: `tests/test_strategies/test_complex_strategy.py`

**Step 1: Write failing test**

```python
# tests/test_strategies/test_complex_strategy.py
import pytest
from eval.src.strategies.complex_strategy import ComplexStrategy


def test_complex_strategy_name():
    strategy = ComplexStrategy(None)
    assert strategy.name == "complex"


def test_complex_strategy_max_length():
    strategy = ComplexStrategy(None)
    assert strategy.max_length == 5000


def test_complex_strategy_uses_fewshot():
    from unittest.mock import Mock
    improver = Mock()
    improver.knn_fewshot_improve = Mock(return_value="Comprehensive improved prompt")

    strategy = ComplexStrategy(improver)
    result = strategy.improve("design full authentication system", "with OAuth and JWT")
    assert result == "Comprehensive improved prompt"
```

**Step 2: Run test to verify it fails**

```bash
pytest tests/test_strategies/test_complex_strategy.py::test_complex_strategy_name -v
```
Expected: FAIL

**Step 3: Implement ComplexStrategy**

```python
# eval/src/strategies/complex_strategy.py
from .base import PromptImproverStrategy


class ComplexStrategy(PromptImproverStrategy):
    """
    KNN Few-shot strategy for complex prompts.

    Characteristics:
    - KNN Few-shot learning (k=3 by default)
    - 5000 character output limit (comprehensive)
    - Lazy compilation (compiles on first use)
    - Best for: full systems, complex architectures
    """

    @property
    def name(self) -> str:
        return "complex"

    @property
    def max_length(self) -> int:
        return 5000

    def _improve_with_strategy(self, original_idea: str, context: str) -> str:
        """
        Use KNN Few-shot DSPy improvement.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            Comprehensive improved prompt with few-shot learning
        """
        try:
            return self._improver.knn_fewshot_improve(original_idea, context)
        except Exception as e:
            # Graceful fallback: if few-shot fails, use moderate
            if "trainset" in str(e).lower() or "compile" in str(e).lower():
                return self._improver.chain_of_thought_improve(original_idea, context)
            raise
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_strategies/test_complex_strategy.py -v
```
Expected: PASS

**Step 5: Commit**

```bash
git add eval/src/strategies/complex_strategy.py tests/test_strategies/test_complex_strategy.py
git commit -m "feat: add ComplexStrategy with KNN Few-shot learning

- Comprehensive strategy with 5000 char limit
- KNN Few-shot approach (k=3) for high-quality output
- Graceful fallback to Chain-of-Thought if trainset unavailable
- Best for full system designs and complex architectures

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 6: Create StrategySelector for routing

**Files:**
- Create: `eval/src/strategy_selector.py`
- Test: `tests/test_strategy_selector.py`

**Step 1: Write failing test**

```python
# tests/test_strategy_selector.py
import pytest
from eval.src.strategy_selector import StrategySelector
from eval.src.complexity_analyzer import ComplexityLevel


def test_selector_has_all_strategies():
    selector = StrategySelector()
    assert selector.simple_strategy is not None
    assert selector.moderate_strategy is not None
    assert selector.complex_strategy is not None


def test_selector_returns_simple_for_simple_input():
    selector = StrategySelector()
    strategy = selector.select("hola mundo", "")
    assert strategy.name == "simple"


def test_selector_returns_moderate_for_moderate_input():
    selector = StrategySelector()
    strategy = selector.select("create button component", "")
    assert strategy.name == "moderate"


def test_selector_returns_complex_for_complex_input():
    selector = StrategySelector()
    long_input = "design full system " * 20
    strategy = selector.select(long_input, "with authentication")
    assert strategy.name == "complex"


def test_selector_get_complexity():
    selector = StrategySelector()
    complexity = selector.get_complexity("hola mundo", "")
    assert complexity == ComplexityLevel.SIMPLE
```

**Step 2: Run tests to verify they fail**

```bash
pytest tests/test_strategy_selector.py -v
```
Expected: FAIL

**Step 3: Implement StrategySelector**

```python
# eval/src/strategy_selector.py
from .complexity_analyzer import ComplexityAnalyzer, ComplexityLevel
from .strategies import SimpleStrategy, ModerateStrategy, ComplexStrategy


class StrategySelector:
    """
    Selects appropriate strategy based on prompt complexity.

    Uses ComplexityAnalyzer to determine input complexity and returns
    the corresponding strategy instance for prompt improvement.
    """

    def __init__(self):
        """Initialize selector with all three strategies."""
        # Strategies will be initialized with PromptImprover instances later
        self._analyzer = ComplexityAnalyzer()
        self._simple_strategy = None
        self._moderate_strategy = None
        self._complex_strategy = None

    @property
    def simple_strategy(self):
        """Lazy-loaded SimpleStrategy."""
        if self._simple_strategy is None:
            from eval.src.dspy_prompt_improver import PromptImprover
            improver = PromptImprover()
            self._simple_strategy = SimpleStrategy(improver)
        return self._simple_strategy

    @property
    def moderate_strategy(self):
        """Lazy-loaded ModerateStrategy."""
        if self._moderate_strategy is None:
            from eval.src.dspy_prompt_improver import PromptImprover
            improver = PromptImprover()
            self._moderate_strategy = ModerateStrategy(improver)
        return self._moderate_strategy

    @property
    def complex_strategy(self):
        """Lazy-loaded ComplexStrategy."""
        if self._complex_strategy is None:
            from eval.src.dspy_prompt_improver import PromptImprover
            improver = PromptImprover()
            self._complex_strategy = ComplexStrategy(improver)
        return self._complex_strategy

    def get_complexity(self, original_idea: str, context: str) -> ComplexityLevel:
        """
        Analyze prompt complexity.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            ComplexityLevel classification
        """
        return self._analyzer.analyze(original_idea, context)

    def select(self, original_idea: str, context: str):
        """
        Select appropriate strategy based on complexity.

        Args:
            original_idea: The user's input prompt
            context: Additional context or requirements

        Returns:
            Appropriate PromptImproverStrategy instance

        Raises:
            ValueError: If complexity level is unrecognized
        """
        complexity = self.get_complexity(original_idea, context)

        strategy_map = {
            ComplexityLevel.SIMPLE: self.simple_strategy,
            ComplexityLevel.MODERATE: self.moderate_strategy,
            ComplexityLevel.COMPLEX: self.complex_strategy,
        }

        strategy = strategy_map.get(complexity)
        if strategy is None:
            raise ValueError(f"No strategy for complexity level: {complexity}")

        return strategy
```

**Step 4: Run tests to verify they pass**

```bash
pytest tests/test_strategy_selector.py -v
```
Expected: PASS (after PromptImprover exists)

**Step 5: Commit**

```bash
git add eval/src/strategy_selector.py tests/test_strategy_selector.py
git commit -m "feat: add StrategySelector for intelligent routing

- Route prompts to appropriate strategy based on complexity
- Lazy-load strategies for efficiency
- Add get_complexity() method for direct access
- Support SIMPLE, MODERATE, COMPLEX routing

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 7: Create minimal DSPy PromptImprover (placeholder)

**Files:**
- Create: `eval/src/dspy_prompt_improver.py`
- Create: `eval/src/__init__.py`

**Step 1: Write minimal DSPy wrapper**

```python
# eval/src/dspy_prompt_improver.py
"""
DSPy-based prompt improver with multiple strategies.
"""
import dspy
from typing import Optional


class PromptImprover:
    """
    DSPy PromptImprover with zero-shot, Chain-of-Thought, and KNN Few-shot.
    """

    def __init__(self):
        """Initialize DSPy with configured LM."""
        # LM should be configured via dspy.configure() before use
        self.lm = dspy.settings.lm

    def zero_shot_improve(self, original_idea: str, context: str) -> str:
        """
        Zero-shot prompt improvement (no examples).

        Args:
            original_idea: User's input prompt
            context: Additional context

        Returns:
            Improved prompt
        """
        signature = dspy.Signature("original_idea, context -> improved_prompt")
        improver = dspy.Predict(signature)
        result = improver(original_idea=original_idea, context=context)
        return result.improved_prompt

    def chain_of_thought_improve(self, original_idea: str, context: str) -> str:
        """
        Chain-of-Thought prompt improvement (reasoning).

        Args:
            original_idea: User's input prompt
            context: Additional context

        Returns:
            Improved prompt with reasoning
        """
        signature = dspy.Signature("original_idea, context -> improved_prompt")
        improver = dspy.ChainOfThought(signature)
        result = improver(original_idea=original_idea, context=context)
        return result.improved_prompt

    def knn_fewshot_improve(self, original_idea: str, context: str) -> str:
        """
        KNN Few-shot prompt improvement (learning from examples).

        Args:
            original_idea: User's input prompt
            context: Additional context

        Returns:
            Improved prompt with few-shot learning
        """
        # KNNFewShot requires trainset - will be configured externally
        signature = dspy.Signature("original_idea, context -> improved_prompt")
        improver = dspy.KNNFewShot(signature, k=3)
        result = improver(original_idea=original_idea, context=context)
        return result.improved_prompt
```

**Step 2: Update eval package exports**

```python
# eval/src/__init__.py
from .complexity_analyzer import ComplexityAnalyzer, ComplexityLevel
from .strategy_selector import StrategySelector
from .dspy_prompt_improver import PromptImprover

__all__ = [
    'ComplexityAnalyzer',
    'ComplexityLevel',
    'StrategySelector',
    'PromptImprover',
]
```

**Step 3: Commit**

```bash
git add eval/src/dspy_prompt_improver.py eval/src/__init__.py
git commit -m "feat: add DSPy PromptImprover with multiple strategies

- Zero-shot: Simple, fast improvements
- Chain-of-Thought: Structured reasoning
- KNN Few-shot: Learning from examples (k=3)
- Abstract DSPy complexity for extensibility

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 8: Create hemdov package structure (minimal)

**Files:**
- Create: `hemdov/__init__.py`
- Create: `hemdov/domain/__init__.py`
- Create: `hemdov/infrastructure/__init__.py`
- Create: `hemdov/interfaces.py`

**Step 1: Create placeholder structure**

```python
# hemdov/__init__.py
"""
Hexagonal Architecture for DSPy Prompt Improvement.

Domain: Business logic (entities, repositories, use cases)
Infrastructure: External concerns (DSPy, SQLite, config)
Interfaces: Dependency injection container
"""

# hemdov/domain/__init__.py
"""Domain layer - business logic."""

# hemdov/infrastructure/__init__.py
"""Infrastructure layer - external integrations."""

# hemdov/interfaces.py
"""Dependency injection container."""
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    """DI container for the application."""

    # Configuration will be added here
    pass

container = Container()
```

**Step 2: Commit**

```bash
git add hemdov/
git commit -m "feat: add hemdov hexagonal architecture structure

- Set up domain, infrastructure, interfaces packages
- Add dependency injection container
- Prepare for DSPy and infrastructure integration

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 9: Update API to use StrategySelector

**Files:**
- Modify: `api/prompt_improver_api.py:137-206`

**Step 1: Add get_strategy_selector function**

```python
# Add after line 151 in api/prompt_improver_api.py

from eval.src.strategy_selector import StrategySelector

# Global selector instance
_strategy_selector: Optional[StrategySelector] = None


def get_strategy_selector() -> StrategySelector:
    """
    Get or initialize StrategySelector with all three strategies.

    Lazy-loads on first call. Handles initialization errors gracefully.

    Returns:
        StrategySelector: Configured selector with Simple, Moderate, Complex strategies
    """
    global _strategy_selector

    if _strategy_selector is None:
        try:
            selector = StrategySelector()
            _strategy_selector = selector
            logger.info("StrategySelector initialized with SimpleStrategy, ModerateStrategy, ComplexStrategy")
        except Exception as e:
            logger.critical(f"Failed to initialize StrategySelector: {e}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail="Failed to initialize prompt strategies. Please try again."
            )

    return _strategy_selector
```

**Step 2: Update improve_prompt endpoint (around line 189-206)**

Replace the current backend selection logic with:

```python
    # Use StrategySelector for intelligent strategy routing
    selector = get_strategy_selector()

    try:
        strategy = selector.select(request.idea, request.context)
        complexity = selector.get_complexity(request.idea, request.context)

        logger.info(
            f"Selected strategy: {strategy.name} | "
            f"complexity: {complexity.value} | "
            f"idea_length: {len(request.idea)}"
        )

        # Call the strategy's improve method
        improved_text = strategy.improve(request.idea, request.context)

        # Validate strategy result
        if improved_text is None or not isinstance(improved_text, str):
            logger.error(f"Strategy {strategy.name} returned invalid result")
            raise HTTPException(
                status_code=500,
                detail="Strategy failed to produce valid output. Please try rephrasing."
            )

        # Check if strategy has improve method (runtime validation)
        if not hasattr(strategy, 'improve') or not callable(strategy.improve):
            logger.error(f"Selected strategy has no improve method | strategy={type(strategy).__name__}")
            raise HTTPException(
                status_code=500,
                detail="Internal error: invalid strategy configuration."
            )

        backend = strategy.name

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in strategy execution: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="An unexpected error occurred. Please try again."
        )
```

**Step 3: Commit**

```bash
git add api/prompt_improver_api.py
git commit -m "feat: integrate StrategySelector into API endpoint

- Replace simple router with intelligent complexity-based routing
- Add get_strategy_selector() with lazy initialization
- Add strategy result validation
- Add comprehensive error handling and logging
- Log strategy selection and complexity for observability

Resolves: Tasks 3, 4, 5 from silent failures plan

Refs: code-review issues #3 (StrategySelector init), #4 (health check context), #5 (validation), #6 (timeout)

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 10: Add timeout to strategy.improve() call

**Files:**
- Modify: `api/prompt_improver_api.py` (in the improve endpoint)

**Step 1: Add timeout wrapper**

Import asyncio at top if not present:
```python
import asyncio
```

Update the strategy call in improve_prompt endpoint:

```python
        # Call the strategy's improve method with timeout
        STRATEGY_TIMEOUT_SECONDS = 60

        try:
            # Run synchronous strategy.improve in thread with timeout
            improved_text = await asyncio.wait_for(
                asyncio.to_thread(strategy.improve, request.idea, request.context),
                timeout=STRATEGY_TIMEOUT_SECONDS
            )
        except asyncio.TimeoutError:
            logger.critical(
                f"Strategy {strategy.name} timed out after {STRATEGY_TIMEOUT_SECONDS}s | "
                f"idea_length: {len(request.idea)}"
            )
            raise HTTPException(
                status_code=504,  # Gateway Timeout
                detail="Prompt improvement took too long. Please try with a shorter prompt."
            )
```

**Step 2: Commit**

```bash
git add api/prompt_improver_api.py
git commit -m "feat: add 60s timeout to strategy.improve() call

- Use asyncio.wait_for with asyncio.to_thread for timeout
- Return 504 Gateway Timeout on expiration
- Log timeout events with strategy context
- Prevent indefinite hangs from slow LLM responses

Resolves: code-review issue #6 (no timeout handling)

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Task 11: Run full test suite

**Step 1: Run all tests**

```bash
pytest tests/ -v --tb=short
```

Expected: All new tests pass (some pre-existing tests may fail)

**Step 2: Run API integration tests specifically**

```bash
pytest tests/test_api_integration.py -v
```

**Step 3: Commit verification**

```bash
git commit --allow-empty -m "test: verify Strategy Pattern implementation complete

All Strategy Pattern components implemented:
- ComplexityAnalyzer with multi-dimensional scoring
- PromptImproverStrategy base class with validation
- SimpleStrategy (zero-shot, 800 char)
- ModerateStrategy (Chain-of-Thought, 2000 char)
- ComplexStrategy (KNN Few-shot, 5000 char)
- StrategySelector for intelligent routing
- API integration with timeout handling

Test Results:
- ComplexityAnalyzer tests: PASS
- Strategy base class tests: PASS
- Strategy implementation tests: PASS
- StrategySelector tests: PASS

 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

---

## Summary

This plan implements the complete Strategy Pattern architecture in 11 focused tasks:

**Core Components (Tasks 1-6):**
1. ComplexityAnalyzer - Multi-dimensional scoring
2. PromptImproverStrategy base class - Interface and validation
3. SimpleStrategy - Zero-shot, ultra-concise
4. ModerateStrategy - Chain-of-Thought, balanced
5. ComplexStrategy - KNN Few-shot, comprehensive
6. StrategySelector - Intelligent routing

**Integration (Tasks 7-11):**
7. DSPy PromptImprover - Multiple strategy support
8. hemdov package structure - Hexagonal architecture
9. API integration - Replace simple router
10. Timeout handling - 60s limit for strategy calls
11. Full test suite verification

**Estimated Time:** 2-3 hours
**Risk:** Low - Each task is independent and testable
**Dependencies:** Python 3.14, DSPy 2.6, pytest, dependency-injector

**After completion:**
- Tasks 3-5 from silent failures plan can now be completed
- API has intelligent prompt routing based on complexity
- System is ready for production with proper error handling
