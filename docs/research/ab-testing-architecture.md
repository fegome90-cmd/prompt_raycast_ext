# A/B Testing Architecture - Sistema de EvaluaciÃ³n Comparativa

**Prioridad:** ğŸ”´ CRÃTICA - ROI MUY ALTO
**Fuente:** Architect v3.2.0 - `/components/EvaluationSuite.tsx`, `/constants.ts`
**Complejidad:** Alta
**Adaptabilidad:** Alta para Raycast

---

## ğŸ¯ Concepto Core

Sistema completo de testing A/B para comparar mÃºltiples prompts mediante test cases automatizados, criterios de evaluaciÃ³n configurables, scoring AI-powered, y anÃ¡lisis estadÃ­stico con comparaciÃ³n contra baseline.

**El problema que resuelve:**
- Â¿CuÃ¡l prompt funciona mejor para un caso de uso?
- Â¿CÃ³mo medir objetivamente la calidad de un prompt?
- Â¿CÃ³mo comparar variaciones de forma sistemÃ¡tica?
- Â¿CuÃ¡ndo una mejora es significativa vs ruido?

**La soluciÃ³n:**
- Suite de evaluaciÃ³n configurable
- Test cases reproducibles
- Criterios de evaluaciÃ³n predefinidos
- Scoring automatizado con AI judge
- AnÃ¡lisis estadÃ­stico (media, desviaciÃ³n estÃ¡ndar)
- ComparaciÃ³n contra baseline

---

## ğŸ—ï¸ Arquitectura del Sistema

### Flujo Principal de EvaluaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     A/B TESTING EVALUATION FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  1. CONFIGURACIÃ“N                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Evaluation Suite                                                  â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Prompts a probar (2+)                                        â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Test cases (inputs)                                          â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Criterios de evaluaciÃ³n                                      â”‚  â”‚
â”‚  â”‚ â””â”€â”€ ConfiguraciÃ³n (model, runs, baseline)                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                                                               â”‚
â”‚                                                                         â”‚
â”‚  2. EJECUCIÃ“N                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ For each test case:                                              â”‚  â”‚
â”‚  â”‚   For each prompt:                                               â”‚  â”‚
â”‚  â”‚     For each run (1-3-5):                                         â”‚  â”‚
â”‚  â”‚       â”œâ”€ Ejecutar prompt con test case                           â”‚  â”‚
â”‚  â”‚       â”œâ”€ Capturar output y mÃ©tricas                              â”‚  â”‚
â”‚  â”‚       â””â”€ Evaluar con AI judge                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                                                               â”‚
â”‚                                                                         â”‚
â”‚  3. ANÃLISIS                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Aggregate Results                                                 â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Score promedio por prompt                                   â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Consistencia (std dev)                                       â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Tiempo de ejecuciÃ³n                                          â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Tokens consumidos                                            â”‚  â”‚
â”‚  â”‚ â””â”€â”€ ComparaciÃ³n vs baseline (delta)                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                                                               â”‚
â”‚                                                                         â”‚
â”‚  4. VISUALIZACIÃ“N                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â”œâ”€â”€ GrÃ¡fico de barras (scores)                                   â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ Tabla comparativa                                             â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ ComparaciÃ³n lado a lado (modal)                              â”‚  â”‚
â”‚  â”‚ â””â”€â”€ Exportar CSV                                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Componentes Clave

### 1. **Evaluation Suite - ConfiguraciÃ³n Central**

**Estructura de datos:**

```typescript
interface EvaluationSuite {
  id: string;
  name: string;
  createdAt: string;

  // QuÃ© probar
  promptIds: string[];           // Prompts a comparar (2+)
  testCases: TestCase[];         // Inputs de prueba
  criteria: EvaluationCriteria[]; // Criterios de evaluaciÃ³n

  // Resultados (post-ejecuciÃ³n)
  results?: EvaluationResult;    // Scores por test case

  // ConfiguraciÃ³n de ejecuciÃ³n
  config: {
    modelForRun: "gemini-2.5-pro" | "gemini-2.5-flash";
    judgeModel: "gemini-2.5-pro" | "gemini-2.5-flash";
    temperature: number;         // 0-1
    baselinePromptId?: string;   // Prompt de referencia
    runsPerTestCase: 1 | 3 | 5;  // Repeticiones para consistencia
  }
}
```

**Concepto clave:** La suite es la unidad de trabajo
- Contiene todo lo necesario para una evaluaciÃ³n
- Es persistente y reutilizable
- Puede ejecutarse mÃºltiples veces
- Almacena resultados histÃ³ricos

### 2. **Test Cases - Inputs Reproducibles**

**Estructura:**

```typescript
interface TestCase {
  id: string;
  input: string;  // El input que se pasa al prompt
}
```

**Patrones de diseÃ±o:**

```
Buenos test cases:
â”œâ”€â”€ EspecÃ­ficos: "Analiza el sentimiento de: 'Me encanta este producto'"
â”œâ”€â”€ Variados: Cubrir edge cases y casos normales
â”œâ”€â”€ Reproducibles: Mismo input = misma evaluaciÃ³n
â””â”€â”€ Independientes: No dependen del orden de ejecuciÃ³n

Malos test cases:
â”œâ”€â”€ Vagos: "algo de sentimiento"
â”œâ”€â”€ Consecutivos: Casos que asumen ejecuciÃ³n previa
â””â”€â”€ Cambiantes: Inputs que varÃ­an cada ejecuciÃ³n
```

**Estrategia de selecciÃ³n:**

1. **Casos normales:** 60-70% de test cases
   - Representan el uso tÃ­pico
   - Buenos resultados esperados

2. **Casos extremos:** 20-30% de test cases
   - Edge cases
   - Inputs inusuales
   - Casos lÃ­mite

3. **Casos negativos:** 10-20% de test cases
   - Inputs invÃ¡lidos
   - Casos que deben fallar
   - Verificar manejo de errores

### 3. **Criterios de EvaluaciÃ³n - Templates Predefinidos**

**10 Templates Predefinidos:**

| ID | Nombre | CategorÃ­a | DescripciÃ³n |
|----|--------|-----------|-------------|
| `clarity` | Clarity and Coherence | clarity | Â¿Es clara y comprensible la respuesta? |
| `actionability` | Actionability | actionability | Â¿Proporciona pasos accionables? |
| `completeness` | Completeness | completeness | Â¿Aborda todos los aspectos? |
| `accuracy` | Accuracy | accuracy | Â¿Es factualmente correcta? |
| `creativity` | Creativity and Innovation | creativity | Â¿Muestra pensamiento innovador? |
| `relevance` | Relevance | relevance | Â¿Es relevante al input? |
| `structure` | Structure and Organization | structure | Â¿EstÃ¡ bien organizada? |
| `conciseness` | Conciseness | clarity | Â¿Es concisa sin perder detalles? |
| `evidence` | Evidence and Support | completeness | Â¿Incluye evidencia o ejemplos? |
| `audience` | Audience Appropriateness | actionability | Â¿Es apropiada para la audiencia? |

**Uso de criterios:**

```
Estrategia 1: Criterios predefinidos (rÃ¡pido)
â””â”€â”€ Seleccionar 2-3 templates de la lista
    â””â”€â”€ Ventaja: No escribir criterios desde cero
    â””â”€â”€ Ideal: Para evaluaciones rÃ¡pidas

Estrategia 2: Criterios personalizados (especÃ­fico)
â””â”€â”€ Escribir criterios especÃ­ficos del dominio
    â””â”€â”€ Ventaja: EvaluaciÃ³n mÃ¡s precisa
    â””â”€â”€ Ideal: Para casos de uso muy especÃ­ficos

Estrategia 3: HÃ­brido (balanceado)
â””â”€â”€ Combinar templates + personalizados
    â””â”€â”€ 1-2 templates + 1-2 criterios especÃ­ficos
    â””â”€â”€ Mejor de ambos mundos
```

### 4. **Proceso de Scoring - AI Judge**

**Arquitectura de dos modelos:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MODEL FOR RUN (Ejecutor)                                  â”‚
â”‚  - Ejecuta el prompt con el test case                     â”‚
â”‚  - Genera el output a evaluar                             â”‚
â”‚  - Modelo mÃ¡s rÃ¡pido (gemini-2.5-flash)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
                  Output Generado
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JUDGE MODEL (Evaluador)                                   â”‚
â”‚  - Recibe: output + input + criterios                     â”‚
â”‚  - Genera: score (1-10) + justificaciÃ³n                   â”‚
â”‚  - Modelo mÃ¡s capaz (gemini-2.5-pro)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Prompt del Judge:**

```
Rol: Eres un evaluador experto en calidad de respuestas AI

Input del test case: "{testCase.input}"

Criterios a evaluar:
1. "{criteria[0].description}"
2. "{criteria[1].description}"
...

Output a evaluar:
"""
{output}
"""

Tu tarea:
1. Evaluar el output contra cada criterio
2. Asignar un score del 1 al 10
3. Justificar tu puntuaciÃ³n

Formato de respuesta JSON:
{
  "score": <nÃºmero 1-10>,
  "justification": "<explicaciÃ³n>"
}
```

**Estrategia de Scoring:**

- **1-3:** Pobre - No cumple el criterio
- **4-6:** Aceptable - Cumple parcialmente
- **7-8:** Bueno - Cumple bien el criterio
- **9-10:** Excelente - Supera expectativas

### 5. **MÃºltiples Runs - Consistencia**

**Por quÃ© mÃºltiples ejecuciones:**

```
Single Run (riesgoso):
Prompt A â”€â”€â–º Score: 7.3
Prompt B â”€â”€â–º Score: 7.8
Â¿Diferencia real? O ruido del modelo?


Multiple Runs (confiable):
Prompt A â”€â”€â–º [7.2, 7.4, 7.3] â†’ Media: 7.30, SD: 0.10
Prompt B â”€â”€â–º [7.1, 8.5, 7.8] â†’ Media: 7.80, SD: 0.70
                                    â†‘
                         Mayor variaciÃ³n = menos confiable
```

**ConfiguraciÃ³n de runs:**

| Runs | Caso de uso | Costo | Confianza |
|------|-------------|-------|-----------|
| 1 | Pruebas rÃ¡pidas | Bajo | Baja |
| 3 | Balance costo/confianza | Medio | Media |
| 5 | MÃ¡xima confianza | Alto | Alta |

**AnÃ¡lisis estadÃ­stico:**

```
Media (promedio): Î¼ = Î£x / n
  - Representa el rendimiento esperado

DesviaciÃ³n EstÃ¡ndar: Ïƒ = âˆš(Î£(x-Î¼)Â² / n)
  - Baja Ïƒ = Consistente
  - Alta Ïƒ = Variable (menos confiable)
```

### 6. **Baseline Comparison - Delta Indicators**

**Concepto de Baseline:**

```
Prompt A (Baseline) â†’ Score: 7.0
Prompt B (Variante) â†’ Score: 7.5 (+0.5 â–²)
Prompt C (Variante) â†’ Score: 6.3 (-0.7 â–¼)
```

**Indicadores Delta:**

```
Î” Score (mayor es mejor):
â”œâ”€â”€ â–² +0.5 = Mejora significativa
â”œâ”€â”€ Â±0.0 = Sin diferencia
â””â”€â”€ â–¼ -0.5 = Empeoramiento

Î” Time (menor es mejor):
â”œâ”€â”€ â–² -200ms = MÃ¡s rÃ¡pido (mejor)
â”œâ”€â”€ Â±0ms = Igual velocidad
â””â”€â”€ â–¼ +200ms = MÃ¡s lento (peor)

Î” Tokens (menor es mejor):
â”œâ”€â”€ â–² -100 = MÃ¡s eficiente
â”œâ”€â”€ Â±0 = Igual consumo
â””â”€â”€ â–¼ +100 = MÃ¡s costoso
```

---

## ğŸ“Š AnÃ¡lisis de Resultados

### 1. **Aggregate Metrics**

**Por Prompt:**

```typescript
interface PromptMetrics {
  promptId: string;
  promptName: string;

  // Score metrics
  avgScore: number;      // Media de todos los runs
  consistency: number;   // DesviaciÃ³n estÃ¡ndar

  // Performance metrics
  avgTime: number;       // Tiempo de ejecuciÃ³n promedio (ms)
  avgTokens: number;     // Tokens consumidos promedio

  // Comparison (si hay baseline)
  deltaScore?: number;   // Diferencia vs baseline
  deltaTime?: number;    // Diferencia de tiempo vs baseline
  deltaTokens?: number;  // Diferencia de tokens vs baseline
}
```

### 2. **VisualizaciÃ³n de Resultados**

**GrÃ¡fico de Barras:**

```
Prompt A (Baseline): â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7.30
Prompt B:           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7.80 â–² +0.5
Prompt C:           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   6.30 â–¼ -1.0

Colores:
â”œâ”€â”€ <3.0: Rojo (pobre)
â”œâ”€â”€ 3-5: Naranja (aceptable)
â”œâ”€â”€ 5-7: Amarillo (bueno)
â”œâ”€â”€ 7-9: Verde claro (muy bueno)
â””â”€â”€ 9-10: Verde oscuro (excelente)
```

**Tabla Comparativa:**

| Prompt | Avg Score | Consistency (SD) | Avg Time (ms) | Avg Tokens |
|--------|-----------|------------------|---------------|------------|
| Prompt A | 7.30 | 0.10 | 1200 | 4500 |
| Prompt B | 7.80 â–² +0.5 | 0.70 â–² +0.6 | 1500 â–² +300 | 4800 â–² +300 |

**Indicadores Delta:**
- â–² = Mejora (verde)
- â–¼ = Empeor (rojo)
- Â± = Sin cambio (gris)

### 3. **Detailed Results - Por Test Case**

```
Test Case: "Analiza el sentimiento de: 'Me encanta este producto'"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt          â”‚ Score    â”‚ Justification                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prompt A        â”‚ 8.2      â”‚ Excelente anÃ¡lisis de        â”‚
â”‚                 â”‚          â”‚ sentimiento positivo con     â”‚
â”‚                 â”‚          â”‚ evidencia clara.            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Prompt B        â”‚ 7.5      â”‚ Buen anÃ¡lisis pero podrÃ­a    â”‚
â”‚                 â”‚          â”‚ incluir mÃ¡s detalles sobre  â”‚
â”‚                 â”‚          â”‚ el tono emocional.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BotÃ³n "Compare Outputs":**
- Abre modal lado a lado
- Muestra todos los outputs
- Facilita comparaciÃ³n cualitativa

### 4. **Export CSV - AnÃ¡lisis Externo**

**Columnas exportadas:**

```csv
test_case_id,test_case_input,prompt_id,prompt_name,output,score,
justification,execution_time_ms,input_tokens,output_tokens,total_tokens
```

**Uso del CSV:**
- AnÃ¡lisis en Excel/Google Sheets
- VisualizaciÃ³n en herramientas de BI
- Machine Learning sobre resultados
- Reportes para stakeholders

---

## ğŸ’¡ AplicaciÃ³n a Raycast

### AdaptaciÃ³n del Concepto

**Para Extension Commands Testing:**

```
Raycast A/B Testing Suite
â”œâ”€â”€ Commands a probar (2+ versiones)
â”‚   â”œâ”€â”€ Command v1: bÃºsqueda simple
â”‚   â””â”€â”€ Command v2: bÃºsqueda con filtros
â”‚
â”œâ”€â”€ Test cases (inputs del usuario)
â”‚   â”œâ”€â”€ "buscar issue #123"
â”‚   â”œâ”€â”€ "buscar pr de felipe"
â”‚   â””â”€â”€ "buscar repositorio raycast"
â”‚
â”œâ”€â”€ Criterios de evaluaciÃ³n
â”‚   â”œâ”€â”€ PrecisiÃ³n del resultado
â”‚   â”œâ”€â”€ Velocidad de respuesta
â”‚   â””â”€â”€ Formato de salida
â”‚
â””â”€â”€ ConfiguraciÃ³n
    â”œâ”€â”€ Modo: local vs API
    â”œâ”€â”€ Runs: 3 por test case
    â””â”€â”€ Baseline: Command v1
```

**MÃ©tricas especÃ­ficas para Raycast:**

| MÃ©trica | CÃ³mo medir | Umbral |
|---------|-----------|--------|
| **PrecisiÃ³n** | Â¿Encuentra lo que busca? | >90% |
| **Velocidad** | Tiempo de respuesta | <2s |
| **UX** | Â¿Es fÃ¡cil usar el resultado? | Score >7 |
| **Eficiencia** | Tokens/API calls mÃ­nimos | <1000 |

### Ejemplo PrÃ¡ctico

**Test Case:** GitHub Issue Search

```
Command v1:
â”œâ”€â”€ Input: "issue #123 en repo X"
â”œâ”€â”€ Output: Lista de issues coincidentes
â””â”€â”€ Criterio: Â¿Encuentra el issue correcto?

Command v2 (con ML):
â”œâ”€â”€ Input: "issue #123 en repo X"
â”œâ”€â”€ Output: Issue mÃ¡s probable + contexto
â””â”€â”€ Criterio: Â¿Es mÃ¡s preciso que v1?

EjecuciÃ³n:
â”œâ”€â”€ 10 test cases (inputs variados)
â”œâ”€â”€ 3 runs por case (consistencia)
â””â”€â”€ Criterio: PrecisiÃ³n + Velocidad

Resultado:
â”œâ”€â”€ v1: PrecisiÃ³n 75%, Velocidad 1.2s
â””â”€â”€ v2: PrecisiÃ³n 92%, Velocidad 1.8s

DecisiÃ³n: v2 es mÃ¡s preciso pero mÃ¡s lento
â†’ Implementar v2 con opciÃ³n de fallback a v1
```

---

## ğŸš€ Decisiones de DiseÃ±o

### Por quÃ© Dos Modelos (Runner + Judge)

**Alternativa considerada:** Usar mismo modelo para todo

**Por quÃ© no:**
- Runner: Necesita ser rÃ¡pido (cost-sensitive)
- Judge: Necesita ser preciso (quality-sensitive)
- Trade-offs Ã³ptimos diferentes

**Por quÃ© dos modelos:**
- **Runner (gemini-2.5-flash):**
  - Ejecuta muchos prompts
  - Velocidad crÃ­tica
  - PrecisiÃ³n aceptable

- **Judge (gemini-2.5-pro):**
  - Ejecuta menos evaluaciones
  - Calidad de scoring crÃ­tica
  - Mejor razonamiento

### Por quÃ© Runs MÃºltiples

**Alternativa considerada:** Single run

**Por quÃ© no:**
- AI models tienen variabilidad inherente
- Single run puede ser outlier
- No se puede medir consistencia

**Por quÃ© mÃºltiples runs:**
- **Promedio:** MÃ¡s representativo del rendimiento real
- **DesviaciÃ³n estÃ¡ndar:** Mide confiabilidad
- **Outliers:** Se pueden identificar y eliminar

**Costo vs beneficio:**
- 1 run: RÃ¡pido pero poco confiable
- 3 runs: Balance Ã³ptimo (recomendado)
- 5 runs: MÃ¡xima confianza pero 5x el costo

### Por quÃ© Baseline

**Alternativa considerada:** Solo scores absolutos

**Por quÃ© no:**
- Score de 7.5 Â¿es bueno o malo?
- Sin contexto, difÃ­cil de interpretar
- Diferencias pueden ser ruido

**Por quÃ© baseline:**
- **Referencia:** ComparaciÃ³n contra conocido
- **Delta:** MediciÃ³n de mejora real
- **DecisiÃ³n:** Â¿Vale la pena el cambio?

---

## ğŸ“ˆ Patrones a Adoptar (Conceptualmente)

### 1. **Suite como Unidad de Trabajo**

```typescript
// NO: Evaluaciones ad-hoc dispersas
evaluate(promptA, testCases)
evaluate(promptB, testCases)

// SÃ: Suite configurada y reutilizable
const suite = {
  prompts: [promptA, promptB],
  testCases: [tc1, tc2, tc3],
  criteria: [clarity, accuracy]
}
runEvaluation(suite)
```

### 2. **SeparaciÃ³n de Responsabilidades**

```
Runner Service:
â””â”€â”€ Ejecuta prompts con test cases
    â””â”€â”€ No sabe de evaluaciÃ³n

Judge Service:
â””â”€â”€ EvalÃºa outputs segÃºn criterios
    â””â”€â”€ No sabe de ejecuciÃ³n

Coordinator:
â””â”€â”€ Orquesta runner + judge
    â””â”€â”€ Conoce ambos servicios
```

### 3. **Resultados Inmutables**

```typescript
// NO: Modificar resultados existentes
suite.results[testCaseId].push(newRun)

// SÃ: Crear nueva versiÃ³n
const newSuite = {
  ...suite,
  results: {
    ...suite.results,
    [testCaseId]: [...suite.results[testCaseId], newRun]
  }
}
```

### 4. **EstimaciÃ³n de Costos**

**Antes de ejecutar:**

```typescript
const estimatedTokens = (
  numTestCases *
  numPrompts *
  runsPerTestCase *
  (avgPromptTokens + avgOutputTokens)
)

// Mostrar al usuario
"Esta evaluaciÃ³n consumirÃ¡ ~50K tokens ($0.15)"
"Â¿Continuar?"
```

---

## âš ï¸ Patrones a Evitar

### 1. **No Asumir Normalidad de Datos**

```typescript
// MAL: Asumir distribuciÃ³n normal
const pValue = calculatePValue(scores) // Requiere normalidad

// BIEN: Usar pruebas no paramÃ©tricas
const winner = sortByMean(scores) // Solo requiere orden
```

### 2. **No Ignorar Consistencia**

```typescript
// MAL: Solo mirar media
if (promptB.avgScore > promptA.avgScore) {
  winner = promptB
}

// BIEN: Considerar consistencia
if (promptB.avgScore > promptA.avgScore &&
    promptB.consistency < MAX_SD) {
  winner = promptB
}
```

### 3. **No Ollear DimensiÃ³n Temporal**

```typescript
// MAL: Un solo punto en el tiempo
const winner = compare(now)

// BIEN: Tendencia en el tiempo
const winner = compare([t1, t2, t3])
```

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito

### Para Medir Calidad de EvaluaciÃ³n

- **Tiempo de ejecuciÃ³n:** <5 min para 10 test cases
- **Costo por evaluaciÃ³n:** <$0.50
- **Consistencia de scoring:** SD < 0.5
- **SatisfacciÃ³n del usuario:** >4/5

### Benchmarks Sugeridos

| MÃ©trica | Bueno | Excelente |
|---------|-------|-----------|
| Test cases por suite | 5-10 | 10+ |
| Prompts por suite | 2-3 | 3+ |
| Tiempo de ejecuciÃ³n | <10 min | <5 min |
| PrecisiÃ³n de scoring | 70% | 85%+ |
| Tasa de re-ejecuciÃ³n | >20% | >50% |

---

## ğŸ” Referencias del CÃ³digo Fuente

### Archivos Principales

| Archivo | PropÃ³sito | LÃ­neas clave |
|---------|-----------|--------------|
| `/components/EvaluationSuite.tsx` | Suite de evaluaciÃ³n A/B | 383-1211 |
| `/constants.ts` | Templates de criterios | 57-118 |
| `/services/geminiService.ts` | Runner y Judge AI | evaluatePromptOutput, runSotaPrompt |

### Secciones Clave

- **Cost estimation:** 100-167 (EvaluationCostEstimator)
- **Aggregate metrics:** 169-307 (AggregateResultsCard)
- **Execution loop:** 527-625 (handleRunEvaluation)
- **CSV export:** 645-709 (handleExportCsv)

---

## âœ… Checklist de ImplementaciÃ³n

Para implementar este patrÃ³n en Raycast:

- [ ] Definir estructura de EvaluationSuite
- [ ] Crear configuraciÃ³n de test cases
- [ ] Implementar sistema de criterios
- [ ] Configurar runner service (ejecutar comandos)
- [ ] Configurar judge service (evaluar resultados)
- [ ] Implementar anÃ¡lisis estadÃ­stico
- [ ] Crear visualizaciÃ³n de resultados
- [ ] AÃ±adir export CSV
- [ ] Implementar comparaciÃ³n baseline
- [ ] Testing de suites mÃºltiples

---

**PrÃ³ximos documentos:**
- `enhancement-engine-pattern.md` - Mejora iterativa automÃ¡tica
- `quality-metrics-system.md` - MÃ©tricas cuantitativas
- `multi-provider-llm-abstraction.md` - AbstracciÃ³n de mÃºltiples LLMs
