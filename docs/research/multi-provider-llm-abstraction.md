# Multi-Provider LLM Abstraction - Capa de AbstracciÃ³n Unificada

**Prioridad:** ðŸ”´ CRÃTICA - ROI MUY ALTO
**Fuente:** Architect v3.2.0 - `/services/llmService.ts`, `/backend/services/llm/LLMServiceManager.js`
**Complejidad:** Alta
**Adaptabilidad:** Requerida para Raycast (Ollama + otros)

---

## ðŸŽ¯ Concepto Core

Capa de abstracciÃ³n que unifica mÃºltiples proveedores de LLM (OpenAI, Anthropic, Google Gemini, GLM, etc.) bajo una interfaz comÃºn, permitiendo cambio dinÃ¡mico de proveedor, fallback automÃ¡tico, selecciÃ³n inteligente por capacidades, y optimizaciÃ³n de costos.

**El problema que resuelve:**
- Â¿CÃ³mo cambiar entre proveedores sin modificar todo el cÃ³digo?
- Â¿QuÃ© hacer si un proveedor falla?
- Â¿CÃ³mo seleccionar el modelo Ã³ptimo para cada tarea?
- Â¿CÃ³mo optimizar costos comparando proveedores?

**La soluciÃ³n:**
- Interfaz unificada para todos los proveedores
- Sistema de registro dinÃ¡mico de proveedores
- SelecciÃ³n inteligente por capacidades
- Fallback automÃ¡tico entre proveedores
- Reintentos con backoff exponencial
- Tracking de costos y performance

---

## ðŸ—ï¸ Arquitectura del Sistema

### Flujo Principal

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-PROVIDER LLM ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  APPLICATION LAYER                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ "Generate text with best model"                                   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â†“                                          â”‚
â”‚  ABSTRACTION LAYER (LLMServiceManager)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ 1. Provider Selection                                               â”‚â”‚
â”‚  â”‚    â”œâ”€ Explicit (user specifies)                                   â”‚â”‚
â”‚  â”‚    â”œâ”€ Default (configured fallback)                               â”‚â”‚
â”‚  â”‚    â””â”€ Intelligent (by capabilities)                               â”‚â”‚
â”‚  â”‚                                                                    â”‚â”‚
â”‚  â”‚ 2. Capability Matching                                             â”‚â”‚
â”‚  â”‚    â”œâ”€ Features required (streaming, vision, etc.)                 â”‚â”‚
â”‚  â”‚    â”œâ”€ Cost constraints                                            â”‚â”‚
â”‚  â”‚    â””â”€ Performance requirements                                     â”‚â”‚
â”‚  â”‚                                                                    â”‚â”‚
â”‚  â”‚ 3. Retry & Fallback Logic                                          â”‚â”‚
â”‚  â”‚    â”œâ”€ Retry with exponential backoff                              â”‚â”‚
â”‚  â”‚    â”œâ”€ Fallback to alternative providers                           â”‚â”‚
â”‚  â”‚    â””â”€ Error categorization (auth, rate-limit, etc.)              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                             â†“                                          â”‚
â”‚  PROVIDER LAYER (Specific Implementations)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Gemini   â”‚  â”‚  OpenAI  â”‚  â”‚Anthropic â”‚  â”‚   GLM    â”‚              â”‚
â”‚  â”‚ Provider â”‚  â”‚ Provider â”‚  â”‚ Provider â”‚  â”‚ Provider â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â”‚
â”‚        â”‚             â”‚             â”‚             â”‚                      â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                             â†“                                          â”‚
â”‚  API LAYER (External Services)                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Google AI â”‚ OpenAI API â”‚ Anthropic API â”‚ GLM API                  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”§ Componentes Clave

### 1. **Model Capabilities Matrix**

**Estructura de capacidades:**

```typescript
interface ModelCapabilities {
  textGeneration: boolean;   // Â¿Genera texto?
  streaming: boolean;        // Â¿Soporta streaming?
  functionCalling: boolean;  // Â¿Soporta function calling?
  imageAnalysis: boolean;    // Â¿Analiza imÃ¡genes?
  longContext: boolean;      // Â¿Soporta contexto largo?
  jsonMode: boolean;         // Â¿Output en JSON?
}
```

**Matriz de modelos:**

| Modelo | Provider | Text | Stream | Func Call | Vision | Long Context | JSON | Cost/1M tokens | Speed |
|--------|----------|------|--------|-----------|--------|--------------|------|---------------|-------|
| Gemini 2.5 Pro | Google | âœ… | âœ… | âœ… | âœ… | âœ… (32K) | âœ… | $1.25 | Medium |
| Gemini 2.5 Flash | Google | âœ… | âœ… | âœ… | âœ… | âœ… (32K) | âŒ | $0.075 | Fast |
| Flash Lite | Google | âœ… | âŒ | âŒ | âŒ | âŒ (8K) | âŒ | $0.0375 | Fast |
| GPT-4 Turbo | OpenAI | âœ… | âœ… | âœ… | âœ… | âœ… (128K) | âœ… | $1.00 | Medium |
| GPT-3.5 Turbo | OpenAI | âœ… | âœ… | âœ… | âŒ | âŒ (16K) | âœ… | $0.50 | Fast |
| Claude 3.5 Sonnet | Anthropic | âœ… | âœ… | âœ… | âœ… | âœ… (200K) | âŒ | $0.003 | Medium |
| Claude 3 Opus | Anthropic | âœ… | âœ… | âœ… | âœ… | âœ… (200K) | âŒ | $0.015 | Slow |
| GLM-4.6 | Zhipu AI | âœ… | âœ… | âœ… | âŒ | âœ… (128K) | âœ… | ~$0.50 | Fast |

**Uso de la matriz:**

```typescript
// SelecciÃ³n por capacidades requeridas
function selectModel(requirements: {
  needsVision: boolean;
  needsStreaming: boolean;
  needsLongContext: boolean;
  maxCostPer1M: number;
}): LLMModel {
  // Filtrar modelos que cumplen requisitos
  const candidates = allModels.filter(model => {
    if (requirements.needsVision && !model.capabilities.imageAnalysis) return false;
    if (requirements.needsStreaming && !model.capabilities.streaming) return false;
    if (requirements.needsLongContext && !model.capabilities.longContext) return false;
    if (model.costPerToken > requirements.maxCostPer1M) return false;
    return true;
  });

  // Ordenar por costo (mÃ¡s barato primero)
  candidates.sort((a, b) => a.costPerToken - b.costPerToken);

  return candidates[0]; // Retorna el mÃ¡s barato que cumple
}
```

### 2. **Provider Registry**

**Sistema de registro dinÃ¡mico:**

```typescript
class LLMServiceManager {
  providers: Map<string, LLMProvider> = new Map();
  defaultProvider: string = null;
  providerPriority: string[] = [];  // Orden de fallback

  // Registro de proveedor
  registerProvider(providerId: string, config: any): LLMProvider {
    let provider;

    switch (providerId) {
      case "gemini":
        provider = new GeminiProvider(config);
        break;
      case "openai":
        provider = new OpenAIProvider(config);
        break;
      case "anthropic":
        provider = new AnthropicProvider(config);
        break;
      case "glm":
        provider = new GLMProvider(config);
        break;
      default:
        throw new Error(`Unknown provider: ${providerId}`);
    }

    // Registrar en el mapa
    this.providers.set(providerId, provider);

    // Primer proveedor registrado â†’ default
    if (!this.defaultProvider) {
      this.defaultProvider = providerId;
    }

    // Agregar a lista de prioridad si no existe
    if (!this.providerPriority.includes(providerId)) {
      this.providerPriority.push(providerId);
    }

    return provider;
  }

  // Configurar orden de fallback
  setProviderPriority(providerIds: string[]) {
    // Validar que todos estÃ©n registrados
    for (const id of providerIds) {
      if (!this.providers.has(id)) {
        throw new Error(`Provider ${id} not registered`);
      }
    }
    this.providerPriority = providerIds;
  }
}
```

**Uso prÃ¡ctico:**

```typescript
// InicializaciÃ³n
const manager = new LLMServiceManager();

manager.registerProvider("gemini", {
  apiKey: process.env.GEMINI_API_KEY
});

manager.registerProvider("openai", {
  apiKey: process.env.OPENAI_API_KEY
});

manager.registerProvider("anthropic", {
  apiKey: process.env.ANTHROPIC_API_KEY
});

// Configurar prioridad de fallback
manager.setProviderPriority([
  "gemini",      // Primero (mÃ¡s barato)
  "openai",      // Segundo (backup)
  "anthropic"    // Tercero (Ãºltimo recurso)
]);
```

### 3. **Unified Request Interface**

**Interfaz Ãºnica para todos los proveedores:**

```typescript
interface LLMRequest {
  // Modelo a usar (opcional - puede ser seleccionado automÃ¡ticamente)
  model?: string;

  // Contenido del prompt
  prompt: PlanData | string;

  // ConfiguraciÃ³n opcional
  systemPrompt?: string;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;

  // Contexto para selecciÃ³n inteligente
  context?: {
    useCase?: string;
    priority?: "speed" | "quality" | "cost";
    requiresStreaming?: boolean;
    requiresVision?: boolean;
    maxCost?: number;
  };
}
```

**Response unificada:**

```typescript
interface LLMResponse {
  content: string;              // Contenido generado
  model: string;                // Modelo usado
  provider: string;             // Proveedor usado
  tokensUsed: {
    input: number;
    output: number;
    total: number;
  };
  cost: number;                 // Costo en USD
  executionTime: number;        // Tiempo en ms
  metrics: {
    averageGenerationSpeed: number;  // tokens/segundo
    firstTokenTime: number;          // tiempo al primer token
  };
  metadata?: {
    reasoning?: string;
    confidence?: number;
    sources?: any[];
  };
}
```

### 4. **Retry Logic with Exponential Backoff**

**Algoritmo de reintento:**

```typescript
async _generateWithRetry(providerId: string, request: LLMRequest): Promise<LLMResponse> {
  const provider = this.getProvider(providerId);
  const maxRetries = 3;
  const baseDelay = 2000;  // 2 segundos
  let lastError;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      // Intentar generaciÃ³n
      return await provider.generateText(request);

    } catch (error) {
      lastError = error;

      // No reintentar en errores irrecuperables
      if (error instanceof AuthenticationError ||
          error instanceof InvalidRequestError ||
          error instanceof ModelNotFoundError) {
        throw error;  // Fallar inmediatamente
      }

      // Para rate limits, respetar retry-after
      if (error instanceof RateLimitError && error.retryAfter) {
        const delay = parseInt(error.retryAfter) * 1000;
        await sleep(delay);
        continue;
      }

      // Exponential backoff para otros errores
      if (attempt < maxRetries - 1) {
        const delay = baseDelay * Math.pow(2, attempt);
        // Intento 1: 2000ms
        // Intento 2: 4000ms
        // Intento 3: 8000ms
        await sleep(delay);
      }
    }
  }

  throw lastError;
}
```

**Matriz de decisiones de reintento:**

| Error Type | Â¿Reintentar? | RazÃ³n |
|------------|--------------|---------|
| `AuthenticationError` | âŒ No | Credenciales invÃ¡lidas no se corrigen solas |
| `InvalidRequestError` | âŒ No | Request mal formado no mejora al reintentar |
| `ModelNotFoundError` | âŒ No | Modelo inexistente no aparece mÃ¡gicamente |
| `RateLimitError` | âœ… SÃ­ | Con retry-after delay |
| `NetworkError` | âœ… SÃ­ | Con backoff exponencial |
| `TimeoutError` | âœ… SÃ­ | Con backoff exponencial |

### 5. **Fallback System**

**Cascada de proveedores:**

```typescript
async _generateWithFallback(request: LLMRequest, failedProviderId: string): Promise<LLMResponse> {
  // Probar cada proveedor en orden de prioridad
  for (const providerId of this.providerPriority) {
    // Saltar el que ya fallÃ³
    if (providerId === failedProviderId) continue;

    // Verificar que estÃ© registrado
    if (!this.providers.has(providerId)) continue;

    try {
      // Intentar con este proveedor
      const result = await this._generateWithRetry(providerId, {
        ...request,
        provider: providerId  // Sobrescribir proveedor
      });

      // Ã‰xito - loggear y retornar
      logger.info(`Fallback successful with provider: ${providerId}`);
      return result;

    } catch (error) {
      // FallÃ³ - probar siguiente
      logger.warn(`Fallback provider ${providerId} also failed`);
      continue;
    }
  }

  // Todos fallaron
  throw new Error("All providers failed");
}
```

**Ejemplo de cascada:**

```
Request â†’ Gemini (primary)
         â†“ fails (RateLimit)
         â†“
         OpenAI (fallback 1)
         â†“ fails (Network Error)
         â†“
         Anthropic (fallback 2)
         â†“ success
         â†“
         Response (from Anthropic)
```

---

## ðŸŽ¯ Intelligent Provider Selection

### SelecciÃ³n por Prioridad

```typescript
enum Priority {
  SPEED = "speed",        // MÃ¡s rÃ¡pido primero
  QUALITY = "quality",    // Mejor calidad primero
  COST = "cost"          // MÃ¡s barato primero
}

function selectModelByPriority(priority: Priority, requirements: Capabilities): LLMModel {
  const candidates = allModels.filter(m => meetsCapabilities(m, requirements));

  switch (priority) {
    case Priority.SPEED:
      // Ordenar por velocidad (fast â†’ medium â†’ slow)
      const speedOrder = { fast: 1, medium: 2, slow: 3 };
      return candidates.sort((a, b) => speedOrder[a.speed] - speedOrder[b.speed])[0];

    case Priority.QUALITY:
      // Ordenar por calidad (heurÃ­stica: contexto largo + reputaciÃ³n)
      return candidates.sort((a, b) => {
        const aScore = (a.capabilities.longContext ? 1 : 0) +
                       (a.provider === "anthropic" ? 0.5 : 0);
        const bScore = (b.capabilities.longContext ? 1 : 0) +
                       (b.provider === "anthropic" ? 0.5 : 0);
        return bScore - aScore;
      })[0];

    case Priority.COST:
      // Ordenar por costo (mÃ¡s barato primero)
      return candidates.sort((a, b) => a.costPerToken - b.costPerToken)[0];
  }
}
```

### Ejemplos de SelecciÃ³n

```
Caso 1: Respuesta rÃ¡pida para chat
â”œâ”€â”€ Priority: SPEED
â”œâ”€â”€ Requirements: streaming=true
â””â”€â”€ Selected: Gemini 2.5 Flash (fast, streaming)

Caso 2: AnÃ¡lisis complejo
â”œâ”€â”€ Priority: QUALITY
â”œâ”€â”€ Requirements: longContext=true
â””â”€â”€ Selected: Claude 3.5 Sonnet (quality, 200K context)

Caso 3: Procesamiento masivo barato
â”œâ”€â”€ Priority: COST
â”œâ”€â”€ Requirements: textGeneration=true
â””â”€â”€ Selected: Gemini Flash Lite ($0.0375/1M)

Caso 4: Con imÃ¡genes
â”œâ”€â”€ Priority: QUALITY
â”œâ”€â”€ Requirements: imageAnalysis=true
â””â”€â”€ Selected: Gemini 2.5 Pro (vision, quality)
```

---

## ðŸ’¡ AplicaciÃ³n a Raycast

### PatrÃ³n para Ollama + Otros Proveedores

**Arquitectura hÃ­brida:**

```typescript
// Interfaz unificada para Raycast
interface RaycastLLMProvider {
  name: string;
  generate(request: LLMRequest): Promise<LLMResponse>;
  generateStreaming(request: LLMRequest): AsyncGenerator<LLMStreamChunk>;
}

// ImplementaciÃ³n Ollama
class OllamaProvider implements RaycastLLMProvider {
  name = "Ollama (Local)";

  async generate(request: LLMRequest): Promise<LLMResponse> {
    const response = await fetch("http://localhost:11434/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: request.model || "llama3.1",
        messages: [
          { role: "system", content: request.systemPrompt || "" },
          { role: "user", content: request.prompt }
        ],
        stream: false,
        temperature: request.temperature || 0.7
      })
    });

    const data = await response.json();

    return {
      content: data.message.content,
      model: request.model || "llama3.1",
      provider: "ollama",
      tokensUsed: {
        input: data.prompt_eval_count || 0,
        output: data.eval_count || 0,
        total: (data.prompt_eval_count || 0) + (data.eval_count || 0)
      },
      cost: 0,  // Gratis (local)
      executionTime: data.total_duration || 0,
      metrics: {
        averageGenerationSpeed: 0,  // Calcular si estÃ¡ disponible
        firstTokenTime: 0
      }
    };
  }

  async *generateStreaming(request: LLMRequest): AsyncGenerator<LLMStreamChunk> {
    const response = await fetch("http://localhost:11434/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: request.model || "llama3.1",
        messages: [
          { role: "system", content: request.systemPrompt || "" },
          { role: "user", content: request.prompt }
        ],
        stream: true
      })
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split("\n").filter(line => line.trim());

      for (const line of lines) {
        if (line.startsWith("data: ")) {
          const data = JSON.parse(line.slice(6));
          if (data.done) {
            yield { content: "", isComplete: true, tokensGenerated: data.eval_count || 0 };
          } else {
            yield { content: data.message.content, isComplete: false, tokensGenerated: 0 };
          }
        }
      }
    }
  }
}

// Servicio unificado para Raycast
class RaycastLLMService {
  private providers = new Map<string, RaycastLLMProvider>();

  constructor() {
    // Registrar Ollama (local, gratis)
    this.providers.set("ollama", new OllamaProvider());

    // Registrar proveedores cloud (opcionales, requieren API key)
    if (process.env.OPENAI_API_KEY) {
      this.providers.set("openai", new OpenAIProvider());
    }
    if (process.env.ANTHROPIC_API_KEY) {
      this.providers.set("anthropic", new AnthropicProvider());
    }
  }

  async generate(request: LLMRequest): Promise<LLMResponse> {
    const providerId = request.provider || this.selectBestProvider(request);

    try {
      return await this.providers.get(providerId).generate(request);

    } catch (error) {
      // Si Ollama falla, intentar con cloud
      if (providerId === "ollama") {
        logger.warn("Ollama failed, trying cloud provider");

        for (const [id, provider] of this.providers) {
          if (id !== "ollama") {
            try {
              return await provider.generate({ ...request, provider: id });
            } catch (e) {
              continue;
            }
          }
        }
      }

      throw error;
    }
  }

  private selectBestProvider(request: LLMRequest): string {
    // Preferir Ollama si estÃ¡ disponible (gratis, local)
    if (this.providers.has("ollama")) {
      return "ollama";
    }

    // Si no, usar el mÃ¡s barato disponible
    // TODO: Implementar lÃ³gica de costo
    return this.providers.keys().next().value;
  }
}
```

### Estrategia de Fallback para Raycast

```
Usuario ejecuta comando de Raycast
        â†“
Intentar Ollama (local, gratis)
        â†“
Â¿Ollama disponible? â”€â”€Noâ†’ Usar OpenAI (cloud, paga)
        â”‚SÃ­
        â†“
Â¿Ollama falla? â”€â”€SÃ­â†’ Fallback a Anthropic
        â”‚No
        â†“
Respuesta exitosa
```

**Beneficios:**
- **Gratis por defecto:** Ollama es local y gratuito
- **Resiliencia:** Si Ollama falla, hay fallback
- **Flexibilidad:** Usuario puede agregar API keys para mÃ¡s opciones
- **Transparencia:** Sistema indica quÃ© proveedor usÃ³

---

## ðŸš€ Decisiones de DiseÃ±o

### Por quÃ© Map en lugar de Array

```typescript
// MAL: Array para proveedores
const providers = [geminiProvider, openaiProvider, anthropicProvider];
const provider = providers.find(p => p.id === "gemini");

// BIEN: Map para lookup O(1)
const providers = new Map([
  ["gemini", geminiProvider],
  ["openai", openaiProvider],
  ["anthropic", anthropicProvider]
]);
const provider = providers.get("gemini");
```

**Ventajas del Map:**
- âœ… Lookup O(1) vs O(n)
- âœ… IDs como claves (mÃ¡s legible)
- âœ… EliminaciÃ³n/adicciÃ³n eficiente
- âœ… No requiere reindexar

### Por why Exponential Backoff

**Alternativa:** Retraso fijo entre reintentos

```typescript
// MAL: Siempre 2 segundos
for (let i = 0; i < 3; i++) {
  try { return await generate(); }
  catch { await sleep(2000); }
}

// BIEN: Backoff exponencial
// Intento 1: inmediato
// Intento 2: 2s despuÃ©s
// Intento 3: 4s despuÃ©s (2^1 * 2000)
// Intento 4: 8s despuÃ©s (2^2 * 2000)
```

**Por quÃ© exponencial:**
- âœ… Da tiempo al servicio para recuperarse
- âœ… No sobrecarga con reintentos agresivos
- âœ… Pat estÃ¡ndar en sistemas distribuidos
- âœ… Balance entre velocidad y persistencia

### Por quÃ© CategorizaciÃ³n de Errores

```typescript
// Error types determinan comportamiento
class AuthenticationError extends Error { }
class RateLimitError extends Error { retryAfter?: string; }
class InvalidRequestError extends Error { }
class ModelNotFoundError extends Error { }

// Retry decision basado en tipo
if (error instanceof AuthenticationError) {
  throw error;  // No reintentar - credenciales invÃ¡lidas
}
if (error instanceof RateLimitError) {
  await sleep(error.retryAfter * 1000);  // Respetar retry-after
  retry();
}
```

---

## ðŸ“ˆ Patrones a Adoptar (Conceptualmente)

### 1. **Interfaz Ãšnica**

```typescript
// Todos los proveedores implementan la misma interfaz
interface LLMProvider {
  generateText(request: LLMRequest): Promise<LLMResponse>;
  generateStreaming(request: LLMRequest): AsyncGenerator<LLMStreamChunk>;
  getModels(): Promise<LLMModel[]>;
  validateApiKey(): Promise<boolean>;
}

// CÃ³digo cliente no necesita saber quÃ© proveedor usa
const response = await llmService.generateText({
  prompt: "Hello, world!",
  provider: "cualquiera"  // O dejar que el sistema elija
});
```

### 2. **Registro DinÃ¡mico**

```typescript
// Proveedores se registran en runtime
llmService.registerProvider("gemini", geminiConfig);
llmService.registerProvider("ollama", ollamaConfig);

// No estÃ¡ hardcodeado
// FÃ¡cil agregar nuevos proveedores
```

### 3. **SelecciÃ³n Basada en Features**

```typescript
const model = llmService.selectModel({
  requiredCapabilities: ["streaming", "longContext"],
  maxCost: 0.002,
  priority: "speed"
});

// Sistema automÃ¡ticamente selecciona el mejor modelo
```

### 4. **Fallback Transparente**

```typescript
// Cliente no maneja fallback
try {
  const response = await llmService.generate(request);
} catch (error) {
  // El servicio ya intentÃ³ todos los proveedores
  // Si llega aquÃ­, todos fallaron
  showError("All AI providers are unavailable");
}

// Fallback es interno al servicio
```

---

## âš ï¸ Patrones a Evitar

### 1. No Asumir Disponibilidad

```typescript
// MAL: Asumir que el proveedor siempre funciona
const result = await geminiProvider.generate(request);

// BIEN: Manejar falla con fallback
try {
  const result = await geminiProvider.generate(request);
} catch (error) {
  const result = await fallbackProvider.generate(request);
}
```

### 2. No Hardcodear Proveedores

```typescript
// MAL: Provider especÃ­fico en cÃ³digo de negocio
if (provider === "gemini") {
  return await callGeminiAPI(request);
} else if (provider === "openai") {
  return await callOpenAIAPI(request);
}

// BIEN: Llamada polimÃ³rfica
const provider = providers.get(providerId);
return await provider.generate(request);
```

### 3. No Ignorar Costos

```typescript
// MAL: Usar el modelo mÃ¡s potente siempre
const model = "claude-3-opus";  // $0.015/1M tokens

// BIEN: Seleccionar por caso de uso
const model = selectBestModel({
  priority: userWantsSpeed ? "speed" : "quality",
  maxCost: userBudget
});
```

---

## ðŸ“ˆ MÃ©tricas de Ã‰xito

### Para Medir Calidad del Servicio

- **Disponibilidad:** >99.5% uptime
- **Tiempo de respuesta:** P50 <500ms, P95 <2s
- **Tasa de Ã©xito:** >95% (incluyendo fallbacks)
- **Costo optimizado:** ReducciÃ³n >50% vs single provider

### Benchmarks Sugeridos

| MÃ©trica | Bueno | Excelente |
|---------|-------|-----------|
| Proveedores soportados | 2+ | 4+ |
| Tiempo de fallback | <2s | <500ms |
| Tasa de Ã©xito con fallback | 95% | 99%+ |
| Ahorro de costos | 30% | 50%+ |

---

## ðŸ” Referencias del CÃ³digo Fuente

### Archivos Principales

| Archivo | PropÃ³sito | LÃ­neas clave |
|---------|-----------|--------------|
| `/services/llmService.ts` | Servicio unificado frontend | 97-300+ |
| `/backend/services/llm/LLMServiceManager.js` | Manager backend | 19-427 |
| `/backend/services/llm/providers/*` | Implementaciones especÃ­ficas | - |

### Funciones Clave

- **Provider Registration:** `registerProvider()` - LLMServiceManager.js:34-75
- **Retry Logic:** `_generateWithRetry()` - LLMServiceManager.js:297-336
- **Fallback:** `_generateWithFallback()` - LLMServiceManager.js:342-380
- **Capability Matching:** `selectBestModel()` - llmService.ts (inferido)

---

**PrÃ³ximos documentos:**
- `validation-pipeline-pattern.md` - Pipeline de validaciÃ³n multi-etapa
- `template-recommendation-strategy.md` - RecomendaciÃ³n por similitud

---

**Documentos completados:**
âœ… `prompt-wizard-pattern.md` - Sistema wizard de 6 pasos
âœ… `ab-testing-architecture.md` - Testing A/B completo
âœ… `enhancement-engine-pattern.md` - Motor de mejora iterativa
âœ… `quality-metrics-system.md` - Sistema cuantitativo de evaluaciÃ³n
âœ… `multi-provider-llm-abstraction.md` - Capa de abstracciÃ³n unificada
