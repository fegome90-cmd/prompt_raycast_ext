# AuditorÃ­a: Pipeline de Prompts - raycast_ext

**Fecha:** 2026-01-02
**Estado:** âœ… Completado
**Objetivo:** AnÃ¡lisis completo del pipeline de mejora de prompts para identificar inconsistencias y oportunidades de optimizaciÃ³n.

---

## 1. Resumen Ejecutivo

El proyecto `raycast_ext` implementa un sistema de mejora de prompts utilizando una arquitectura hÃ­brida con **DSPy como backend principal** y **Ollama como motor de inferencia local**. El sistema no utiliza base de datos persistente, operando de manera completamente stateless.

### Stack TecnolÃ³gico

| Componente | TecnologÃ­a | UbicaciÃ³n |
|------------|-----------|-----------|
| Frontend | TypeScript + Raycast SDK | `dashboard/src/` |
| Backend API | FastAPI + Python | RaÃ­z del proyecto |
| Framework de Prompts | DSPy (Stanford) | `hemdov/domain/dspy_modules/` |
| Motor LLM | Ollama (Local) | `http://localhost:11434` |
| Adaptador Universal | LiteLLM | `hemdov/infrastructure/adapters/` |
| Modelo Primario | Novaeus-Promptist-7B | Modelo GGUF via Ollama |
| Modelo Fallback | devstral:24b | Modelo alternativo |

---

## 2. Arquitectura del Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FLUJO COMPLETO                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usuario (Raycast)
    â†“ [Input: idea cruda]
promptify-quick.tsx
    â†“ [ValidaciÃ³n: min 5 caracteres]
improvePrompt.ts (Frontend Logic)
    â†“
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Intenta DSPy Backend (localhost:8000) â”‚
    â”‚  â†“                                   â”‚
    â”‚  PromptImproverAPI                   â”‚
    â”‚  â†“                                   â”‚
    â”‚  dspy_prompt_improver.py             â”‚
    â”‚  â†“                                   â”‚
    â”‚  LiteLLM Adapter                     â”‚
    â”‚  â†“                                   â”‚
    â”‚  Ollama API                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (si DSPy falla)
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Ollama Directo (Fallback)           â”‚
    â”‚  â†“                                   â”‚
    â”‚  LiteLLM â†’ Ollama                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Output Estructurado]
```

### 2.1 Punto de Entrada

**Archivo:** `dashboard/src/promptify-quick.tsx`

```typescript
// Flujo principal:
1. Usuario abre Raycast extension
2. Input desde selecciÃ³n o clipboard
3. Carga de preferencias de configuraciÃ³n
4. Llamada a improvePromptWithHybrid()
5. Renderizado del resultado
```

### 2.2 LÃ³gica de Mejora

**Archivo:** `dashboard/src/core/llm/improvePrompt.ts`

Estrategia hÃ­brida:
1. **DSPy-First**: Intenta backend DSPy en `localhost:8000`
2. **Fallback**: Si DSPy no responde, usa Ollama directo
3. **Quality Gates**: ValidaciÃ³n de JSON y confidence scoring

### 2.3 Backend DSPy

**Archivo:** `hemdov/domain/dspy_modules/prompt_improver.py`

```python
# PatrÃ³n Architect implementado:
class PromptImprover(dspy.Module):
    """
    Input: raw_idea (str), context (str, optional)
    Output:
        - improved_prompt (str)
        - role (str)
        - directive (str)
        - framework (str)
        - guardrails (list[str])
        - reasoning (str, optional)
        - confidence (float, optional)
    """
```

### 2.4 Adaptador LiteLLM

**Archivo:** `hemdov/infrastructure/adapters/litellm_dspy_adapter.py`

Proveedores soportados:
- Ollama (principal)
- Gemini
- DeepSeek
- OpenAI

---

## 3. ConfiguraciÃ³n y Modelos

### 3.1 Modelos Configurados

```bash
# Modelo primario (especializado en prompts)
MODEL=hf.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF:Q5_K_M

# Modelo fallback (mÃ¡s general)
FALLBACK_MODEL=devstral:24b

# Base URL de Ollama
OLLAMA_BASE_URL=http://localhost:11434

# Base URL del backend DSPy
DSPY_BASE_URL=http://localhost:8000
```

### 3.2 Variables de Entorno DSPy

```bash
DSPY_MAX_BOOTSTRAPPED_DEMOS=5    # MÃ¡ximo de demos generadas
DSPY_MAX_LABELED_DEMOS=3          # MÃ¡ximo de demos etiquetadas
DSPY_COMPILED_PATH=               # Ruta al mÃ³dulo compilado
```

### 3.3 ConfiguraciÃ³n Frontend

```typescript
// Opciones configurables por el usuario:
{
  ollamaBaseUrl: "http://localhost:11434",
  dspyBaseUrl: "http://localhost:8000",
  dspyEnabled: true,              // Habilitar/deshabilitar DSPy
  model: "Novaeus-Promptist-7B...",
  fallbackModel: "devstral:24b",
  preset: "structured | default | specific | coding",
  timeoutMs: 30000
}
```

---

## 4. Patrones de DiseÃ±o

### 4.1 PatrÃ³n Architect

El sistema utiliza el patrÃ³n Architect para estructurar prompts:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ROLE        â†’ QuiÃ©n es el AI                            â”‚
â”‚  DIRECTIVE   â†’ QuÃ© debe hacer                            â”‚
â”‚  FRAMEWORK   â†’ CÃ³mo debe abordar el problema              â”‚
â”‚  GUARDRAILS  â†’ LÃ­mites y restricciones                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Chain-of-Thought (CoT)

DSPy implementa razonamiento paso a paso:
1. AnÃ¡lisis de la idea cruda
2. IdentificaciÃ³n del contexto
3. GeneraciÃ³n de mejoras
4. ValidaciÃ³n estructural
5. Salida formateada

### 4.3 Template-Based

META-PROMPT-UNIVERSAL para estructura consistente:
```typescript
const META_PROMPT = `
Role: {role}
Directive: {directive}
Framework: {framework}
Guardrails: {guardrails}
`;
```

---

## 5. Manejo de Errores y Fallbacks

### 5.1 Estrategia de Fallback

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DSPy Backend        â”‚ â† Intenta primero
â”‚  (localhost:8000)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (falla)
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Directo      â”‚ â† Fallback
â”‚  (localhost:11434)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (falla)
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Error Message       â”‚ â† Ãšltimo recurso
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Estrategias de ExtracciÃ³n JSON

Cuando Ollama devuelve texto no estructurado:

1. **Fenced JSON**: Busca `\`\`\`json ... \`\`\``
2. **Tagged JSON**: Busca `<json>...</json>`
3. **Scanning**: Escaneo por JSON vÃ¡lido
4. **Repair**: ExtracciÃ³n automÃ¡tica si parse falla

### 5.3 Quality Gates

| MÃ©trica | Umbral | PropÃ³sito |
|---------|--------|-----------|
| JSON Valid | â‰¥54% | Respuestas parseables |
| Copyable Rate | â‰¥54% | Prompts usables directamente |
| Latency P95 | â‰¤12s | Tiempo de respuesta mÃ¡ximo |
| Min Confidence | 0.7 | Confianza mÃ­nima requerida |

---

## 6. Almacenamiento de Datos

### 6.1 Sin Base de Datos Persistente

âš ï¸ **Hallazgo CrÃ­tico**: El sistema NO guarda historial de prompts.

**Implicaciones:**
- No hay aprendizaje acumulativo
- No se puede trackear mejoras
- No hay auditorÃ­a de uso
- Cada request es independiente

**Soluciones Posibles:**
1. SQLite local para historial
2. Logs estructurados para anÃ¡lisis
3. Vector DB para bÃºsqueda semÃ¡ntica
4. Cache de prompts mejorados

### 6.2 Almacenamiento Actual

| Tipo | UbicaciÃ³n | Persistencia |
|------|-----------|--------------|
| ConfiguraciÃ³n | `.env` + Raycast Preferences | Permanente |
| Estado Runtime | Memoria | VolÃ¡til |
| Logs | Stdout/Stderr | VolÃ¡til |
| Prompts | Ninguno | âŒ No persiste |

---

## 7. Puntos a Investigar (Inconsistencias Potenciales)

### 7.1 ğŸ”´ CrÃ­ticas

| # | Issue | Impacto | Archivos |
|---|-------|---------|----------|
| 1 | **No hay persistencia de prompts** | Alta | `-` |
| 2 | **DSPy backend no estÃ¡ compilado** (`DSPY_COMPILED_PATH=`) | Alta | `.env` |
| 3 | **No hay monitoreo de mÃ©tricas** | Media | `-` |
| 4 | **Timeout fijo de 30s puede ser insuficiente** | Media | `improvePrompt.ts` |

### 7.2 ğŸŸ¡ Medias

| # | Issue | Impacto | Archivos |
|---|-------|---------|----------|
| 5 | **No hay reintentos automÃ¡ticos** | Media | `improvePrompt.ts` |
| 6 | **Fallback model es mÃ¡s lento (24b vs 7b)** | Media | Config |
| 7 | **No hay validaciÃ³n de inputs del usuario** | Baja | `promptify-quick.tsx` |
| 8 | **Quality gates bajos (54%)** | Baja | `improvePrompt.ts` |

### 7.3 ğŸŸ¢ Mejoras

| # | Sugerencia | Beneficio |
|---|------------|-----------|
| 9 | Agregar logging estructurado | Debugging |
| 10 | Implementar cache de prompts | Performance |
| 11 | Agregar tests unitarios | Confianza |
| 12 | Documentar API endpoints | Mantenibilidad |

---

## 8. Archivos Clave Identificados

### Frontend (TypeScript)

```
dashboard/src/
â”œâ”€â”€ promptify-quick.tsx              # Entry point principal
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ improvePrompt.ts         # LÃ³gica hÃ­brida DSPy/Ollama
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ index.ts                 # Config loader
â”‚       â”œâ”€â”€ defaults.ts              # Valores por defecto
â”‚       â””â”€â”€ schema.ts                # ValidaciÃ³n Zod
â””â”€â”€ components/
    â””â”€â”€ (UI components para prompts)
```

### Backend (Python)

```
hemdov/
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ dspy_modules/
â”‚       â””â”€â”€ prompt_improver.py       # MÃ³dulo DSPy principal
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ adapters/
â”‚       â””â”€â”€ litellm_dspy_adapter.py # Adaptador universal
â””â”€â”€ api/
    â””â”€â”€ prompt_improver_api.py       # FastAPI endpoint

eval/
â””â”€â”€ src/
    â””â”€â”€ dspy_prompt_improver.py      # VersiÃ³n compilada
```

### ConfiguraciÃ³n

```
.
â”œâ”€â”€ .env                             # Variables de entorno
â”œâ”€â”€ pyproject.toml                   # Dependencias Python
â”œâ”€â”€ dashboard/package.json           # Dependencias Node
â””â”€â”€ requirements.txt                 # Requisitos Python
```

---

## 9. Estado de CompilaciÃ³n DSPy

### 9.1 MÃ³dulo Compilado vs No Compilado

El sistema soporta dos modos de operaciÃ³n:

| Modo | DescripciÃ³n | Estado Actual |
|------|-------------|---------------|
| **Compiled** | DSPy con ejemplos optimizados | âŒ No configurado |
| **Zero-shot** | DSPy sin optimizaciÃ³n previa | âœ… Activo |

### 9.2 CompilaciÃ³n

Para compilar el mÃ³dulo DSPy:

```bash
# Dataset de entrenamiento requerido
npm run eval -- --dataset testdata/cases.jsonl --output eval/compiled.json

# Esto generarÃ­a:
# - Few-shot examples optimizados
# - Mejor calidad de salida
# - Mayor consistencia
```

**Estado Actual:** `DSPY_COMPILED_PATH=` (vacÃ­o)

---

## 10. VerificaciÃ³n de Estado (2026-01-02)

### 10.1 Estado de Servicios âœ…

| Servicio | Estado | Detalles |
|----------|--------|----------|
| **DSPy Backend** | âœ… Running | `http://localhost:8000` - Status: healthy, DSPy configured: true |
| **Ollama** | âœ… Running | `http://localhost:11434` - 4 modelos disponibles |
| **Novaeus-Promptist-7B** | âœ… Loaded | Modelo primario presente (5.4GB) |
| **API Endpoint** | âœ… Available | `/api/v1/improve-prompt` funcional |

### 10.2 Modelos Disponibles en Ollama

```
1. hf.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF:Q5_K_M (5.4GB)
   â†’ Modelo especializado en prompt engineering

2. nomic-embed-text:latest (274MB)
   â†’ Modelo para embeddings

3. qwen3-coder:30b (18.5GB)
   â†’ Modelo para cÃ³digo

4. qwen3:latest (5.2GB)
   â†’ Modelo general
```

### 10.3 VerificaciÃ³n de ConfiguraciÃ³n

**Archivo `.env` (lÃ­neas clave):**
```bash
# âœ… Correcto
LLM_PROVIDER=ollama
LLM_MODEL=hf.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF:Q5_K_M
LLM_BASE_URL=http://localhost:11434

# âš ï¸ VacÃ­o - DSPy no estÃ¡ compilado
DSPY_COMPILED_PATH=

# âœ… Configurado
MIN_CONFIDENCE_THRESHOLD=0.7
MAX_LATENCY_MS=30000
```

**Archivo `defaults.ts` (lÃ­neas clave):**
```typescript
// âœ… Quality gates configurados
jsonValidPass1: 0.54,    // 54% mÃ­nimo
copyableRate: 0.54,      // 54% mÃ­nimo
latencyP95Max: 12_000,   // 12 segundos mÃ¡ximo

// âœ… Timeout configurado
timeoutMs: 30_000,       // 30 segundos
```

### 10.4 AnÃ¡lisis del CÃ³digo `improvePrompt.ts`

**Estrategia hÃ­brida confirmada:**
```typescript
// LÃ­nea 74-153: improvePromptWithHybrid()
1. Crea cliente DSPy (localhost:8000)
2. Health check a DSPy backend
3. Si DSPy healthy â†’ usa DSPy
4. Si DSPy falla â†’ fallback a Ollama
5. Retorna metadata con backend usado
```

**Flujo Ollama con 2 intentos:**
```typescript
// LÃ­nea 158-284: improvePromptWithOllama()
Intento 1: GeneraciÃ³n directa
  â†“ Si hay quality issues
Intento 2: Repair prompt
  â†“ Retorna resultado
```

---

## 11. Hallazgos Confirmados

### 11.1 ğŸ”´ CrÃ­ticas Confirmadas

| # | Issue | Estado | Impacto |
|---|-------|--------|---------|
| 1 | **No hay persistencia de prompts** | âœ… Confirmado | Alta |
| 2 | **DSPy backend no estÃ¡ compilado** (`DSPY_COMPILED_PATH=`) | âœ… Confirmado | Alta |
| 3 | **No hay monitoreo de mÃ©tricas** | âœ… Confirmado | Media |

### 11.2 ğŸŸ¡ Medias Confirmadas

| # | Issue | Estado | Impacto |
|---|-------|--------|---------|
| 4 | **No hay reintentos automÃ¡ticos** | âœ… Confirmado | Media |
| 5 | **Quality gates al 54%** | âœ… Confirmado | Baja-Media |

### 11.3 âœ… Componentes Funcionales

| Componente | Estado | Notas |
|------------|--------|-------|
| DSPy Backend | âœ… Operational | Health check OK |
| Ollama API | âœ… Operational | 4 modelos cargados |
| Fallback mechanism | âœ… Working | DSPy â†’ Ollama |
| JSON extraction | âœ… Multi-strategy | 3 mÃ©todos implementados |
| Auto-repair | âœ… Working | 2 intentos con repair |

---

## 12. Inconsistencias Identificadas

### 12.1 Puerto DSPy: Diferencia entre .env y defaults.ts

| Archivo | ConfiguraciÃ³n |
|---------|---------------|
| `.env` | `API_PORT=8001` |
| `defaults.ts` | `baseUrl: "http://localhost:8000"` |

**Impacto:** Si se cambia el puerto en .env, el frontend seguirÃ¡ intentando conectar al 8000.

**RecomendaciÃ³n:** Unificar configuraciÃ³n de puerto.

### 12.2 DSPy Signature vs Output Schema

**DSPy Signature** (`prompt_improver.py`):
```python
- improved_prompt (str)
- role (str)
- directive (str)
- framework (str)
- guardrails (list[str])
- reasoning (str, optional)
- confidence (float, optional)
```

**Frontend Schema** (`improvePrompt.ts`):
```typescript
{
  improved_prompt: string,
  clarifying_questions: string[],  // âŒ No existe en DSPy
  assumptions: string[],           // âŒ No existe en DSPy
  confidence: number
}
```

**Impacto:** DSPy no genera `clarifying_questions` ni `assumptions` - el frontend los setea como array vacÃ­o.

**Estado:** Funcional pero inconsistente.

### 12.3 MÃ³dulo DSPy No Implementado

**Archivo:** `hemdov/domain/dspy_modules/prompt_improver.py`

**Contenido actual:** Solo define la `Signature`, no el `Module`.

```python
# âŒ Falta:
class PromptImprover(dspy.Module):
    def forward(self, original_idea: str, context: str) -> Output:
        # ImplementaciÃ³n no encontrada
```

**Impacto:** El backend DSPy probablemente usa una implementaciÃ³n separada no visible en este archivo.

---

## 13. Recomendaciones Prioritarias

### 13.1 Inmediatas (Alta Prioridad)

1. **Unificar configuraciÃ³n de puerto DSPy**
   - Sincronizar `.env:API_PORT` con `defaults.ts:dspy.baseUrl`

2. **Implementar persistencia bÃ¡sica**
   - SQLite local para historial de prompts
   - Tabla: `prompts(id, input, output, backend, timestamp, quality_score)`

3. **Documentar mÃ³dulo DSPy**
   - Ubicar la implementaciÃ³n real de `PromptImprover.forward()`
   - Documentar por quÃ© `clarifying_questions` no se genera

### 13.2 Corto Plazo (Media Prioridad)

4. **Agregar monitoreo bÃ¡sico**
   - Logging estructurado con timestamps
   - MÃ©tricas: latencia, success rate, backend usado

5. **Compilar DSPy**
   - Ejecutar `npm run eval` para generar few-shot examples
   - Configurar `DSPY_COMPILED_PATH` con el resultado

6. **Implementar reintentos**
   - 2 reintentos automÃ¡ticos antes de fallback
   - Backoff exponencial: 1s â†’ 2s â†’ 4s

### 13.3 Largo Plazo (Baja Prioridad)

7. **Subir quality gates**
   - Actual: 54% â†’ Objetivo: 70%
   - Baseline: medir performance actual

8. **Agregar tests E2E**
   - Test del pipeline completo
   - Mock de DSPy y Ollama

---

## 14. ConclusiÃ³n

El sistema estÃ¡ **funcional y operativo** con DSPy backend y Ollama corriendo correctamente. Las principales inconsistencias identificadas son:

1. **ConfiguraciÃ³n desincronizada** (puerto DSPy)
2. **Falta de persistencia** (no se guarda historial)
3. **DSPy no compilado** (opera en modo zero-shot)
4. **Esquemas inconsistentes** (DSPy vs Frontend)

**Estado General:** ğŸŸ¡ Operativo con oportunidades de mejora

**Riesgo Inmediato:** Bajo - el sistema funciona correctamente

**Deuda TÃ©cnica:** Media - mejorar persistencia y compilaciÃ³n incrementarÃ­a calidad significativamente

---

## 15. Referencias

- **DSPy Documentation:** https://dspy-docs.vercel.app/
- **LiteLLM Documentation:** https://docs.litellm.ai/
- **Ollama Documentation:** https://ollama.com/docs
- **Novaeus Model:** https://huggingface.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF

---

**Ãšltima actualizaciÃ³n:** 2026-01-02
**PrÃ³xima revisiÃ³n:** Pendiente anÃ¡lisis de logs y mÃ©tricas reales
