# DSPy Prompt Improver Environment Configuration
# Copy this file to .env and update values as needed

# ============================================
# LLM Provider Configuration
# ============================================
# Options: ollama, gemini, deepseek, openai, anthropic
# Default: anthropic (Haiku 4.5 - fastest & most cost-effective)
LLM_PROVIDER=anthropic

# Model Configuration
# ================
# Anthropic Claude Models:
#   - claude-haiku-4-5-20251001   (Haiku 4.5 - fastest, ~$0.08/1M tokens) ✓ RECOMMENDED
#   - claude-sonnet-4-5-20250929  (Sonnet 4.5 - balanced, ~$3/1M tokens)
#   - claude-opus-4-20250514      (Opus 4 - highest quality, ~$15/1M tokens)
#
# DeepSeek Models:
#   - deepseek-chat    (Main chat model - lowest cost)
#   - deepseek-reasoner (Reasoning model)
#
# Gemini Models:
#   - gemini-2.0-flash-exp (Fast, experimental)
#   - gemini-2.5-pro      (Latest Pro)
#
# OpenAI Models:
#   - gpt-4o-mini    (Fastest, cheapest)
#   - gpt-4o         (Standard)
#
# Ollama Local Models:
#   - hf.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF:Q5_K_M
#   - llama3.2, mistral, or any model pulled with `ollama pull`
LLM_MODEL=claude-haiku-4-5-20251001

# API Base URL (optional - uses provider defaults if omitted)
# For Anthropic, always use: https://api.anthropic.com
LLM_BASE_URL=https://api.anthropic.com

# ============================================
# API Keys (only required for non-Ollama providers)
# ============================================
# Anthropic API Key (required when LLM_PROVIDER=anthropic)
# Get your key from: https://console.anthropic.com/
# Supports both ANTHROPIC_API_KEY and HEMDOV_ANTHROPIC_API_KEY
ANTHROPIC_API_KEY=your_anthropic_api_key_here
HEMDOV_ANTHROPIC_API_KEY=your_anthropic_api_key_here

# DeepSeek API Key (required when LLM_PROVIDER=deepseek)
# Get your key from: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Gemini API Key (required when LLM_PROVIDER=gemini)
# GEMINI_API_KEY=your_gemini_api_key_here

# OpenAI API Key (required when LLM_PROVIDER=openai)
# OPENAI_API_KEY=your_openai_api_key_here

# Generic API Key (fallback if provider-specific key not set)
# LLM_API_KEY=your_generic_api_key_here

# ============================================
# SQLite Persistence Configuration
# ============================================
# Master switch for prompt history tracking
SQLITE_ENABLED=true

# Database file location (relative to project root)
SQLITE_DB_PATH=data/prompt_history.db

# Connection pool size (SQLite: 1 is optimal due to file-locking)
SQLITE_POOL_SIZE=1

# Auto-delete records older than N days (0 = disable)
SQLITE_RETENTION_DAYS=30

# Run cleanup on startup (removes expired records)
SQLITE_AUTO_CLEANUP=true

# Write-Ahead Logging for better concurrency (recommended)
SQLITE_WAL_MODE=true

# ============================================
# LangChain Hub Integration Configuration
# ============================================
# LangChain API Key (required for importing prompts from LangChain Hub)
# Get your key from: https://smith.langchain.com/settings > API Keys > Create
# Format: lsv2_xxx...
LANGCHAIN_API_KEY=your_langchain_api_key_here

# Enable LangSmith tracing for LangChain operations (optional)
LANGCHAIN_TRACING_V2=true

# ============================================
# DSPy Configuration
# ============================================
DSPY_MAX_BOOTSTRAPPED_DEMOS=5
DSPY_MAX_LABELED_DEMOS=3
DSPY_COMPILED_PATH=

# Few-Shot Learning Configuration
# ============================================
# Master switch for few-shot learning (recommended: true for production)
# USE_KNN_FEWSHOT is the new flag name (takes precedence)
# DSPY_FEWSHOT_ENABLED is the legacy flag name (for backward compatibility)
USE_KNN_FEWSHOT=true
DSPY_FEWSHOT_ENABLED=false

# Path to unified few-shot pool (created by merge_unified_pool.py)
DSPY_FEWSHOT_TRAINSET_PATH=datasets/exports/unified-fewshot-pool.json

# Number of neighbors for KNNFewShot (default: 3)
DSPY_FEWSHOT_K=3

# Path to save/load compiled few-shot model
DSPY_FEWSHOT_COMPILED_PATH=models/fewshot-compiled.json

# ============================================
# API Server Configuration
# ============================================
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# ============================================
# Timeout Configuration (CRITICAL - must sync across layers)
# ============================================
# Anthropic Haiku API timeout (seconds)
# MUST match: Frontend (package.json:120000) → FastAPI (120s) → LiteLLM (120s)
# See: dashboard/docs/logging-guide.md and dashboard/src/core/config/defaults.ts:58-80
# Default: 120 (allows Haiku ~30-50s per request)
ANTHROPIC_TIMEOUT=120

# ============================================
# Quality Thresholds
# ============================================
MIN_CONFIDENCE_THRESHOLD=0.7
MAX_LATENCY_MS=30000
