# DSPy Prompt Improver Environment Configuration
# Copy this file to .env and update values as needed

# ============================================
# LLM Provider Configuration
# ============================================
# Options: ollama, gemini, deepseek, openai
# Default: deepseek (recommended for production)
LLM_PROVIDER=deepseek

# Model Configuration
# For deepseek: deepseek-chat or deepseek-reasoner
# For ollama: any GGUF model (e.g., hf.co/mradermacher/Novaeus-Promptist-7B-Instruct-i1-GGUF:Q5_K_M)
# For gemini: gemini-2.0-flash or gemini-2.5-pro
# For openai: gpt-4o-mini or gpt-4o
LLM_MODEL=deepseek-chat

# API Base URL (optional - uses provider defaults if omitted)
LLM_BASE_URL=https://api.deepseek.com/v1

# ============================================
# API Keys (only required for non-Ollama providers)
# ============================================
# DeepSeek API Key (required when LLM_PROVIDER=deepseek)
# Get your key from: https://platform.deepseek.com/
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Gemini API Key (required when LLM_PROVIDER=gemini)
# GEMINI_API_KEY=your_gemini_api_key_here

# OpenAI API Key (required when LLM_PROVIDER=openai)
# OPENAI_API_KEY=your_openai_api_key_here

# Generic API Key (fallback if provider-specific key not set)
# LLM_API_KEY=your_generic_api_key_here

# ============================================
# DSPy Configuration
# ============================================
DSPY_MAX_BOOTSTRAPPED_DEMOS=5
DSPY_MAX_LABELED_DEMOS=3
DSPY_COMPILED_PATH=

# ============================================
# API Server Configuration
# ============================================
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# ============================================
# Quality Thresholds
# ============================================
MIN_CONFIDENCE_THRESHOLD=0.7
MAX_LATENCY_MS=30000
