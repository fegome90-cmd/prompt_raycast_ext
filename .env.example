# DSPy Prompt Improver Environment Configuration
# Copy this file to .env and update values as needed

# LLM Provider Configuration
# Options: ollama, gemini, deepseek, openai
LLM_PROVIDER=ollama

# Model Configuration
LLM_MODEL=llama3.1
LLM_BASE_URL=http://localhost:11434

# API Keys (only required for non-Ollama providers)
# GEMINI_API_KEY=your_gemini_api_key_here
# DEEPSEEK_API_KEY=your_deepseek_api_key_here
# OPENAI_API_KEY=your_openai_api_key_here

# DSPy Configuration
DSPY_MAX_BOOTSTRAPPED_DEMOS=5
DSPY_MAX_LABELED_DEMOS=3
DSPY_COMPILED_PATH=

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# Quality Thresholds
MIN_CONFIDENCE_THRESHOLD=0.7
MAX_LATENCY_MS=30000